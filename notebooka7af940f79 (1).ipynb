{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-30T18:54:17.547062Z","iopub.execute_input":"2023-03-30T18:54:17.547743Z","iopub.status.idle":"2023-03-30T18:54:17.556927Z","shell.execute_reply.started":"2023-03-30T18:54:17.547706Z","shell.execute_reply":"2023-03-30T18:54:17.555716Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/kaggle/input/call-prediction/sample_submission.csv\n/kaggle/input/call-prediction/train.csv\n/kaggle/input/call-prediction/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/call-prediction/train.csv\")\ntrain","metadata":{"execution":{"iopub.status.busy":"2023-03-30T18:54:17.559233Z","iopub.execute_input":"2023-03-30T18:54:17.559983Z","iopub.status.idle":"2023-03-30T18:54:17.634086Z","shell.execute_reply.started":"2023-03-30T18:54:17.559947Z","shell.execute_reply":"2023-03-30T18:54:17.632945Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                ID  가입일  음성사서함이용  주간통화시간  주간통화횟수  주간통화요금  저녁통화시간  저녁통화횟수  \\\n0      TRAIN_00000  329        0    99.2      93    27.3   268.8      68   \n1      TRAIN_00001    2       80   323.9     323    83.7   269.4     326   \n2      TRAIN_00002   93       28   282.4     323    34.2   207.0     322   \n3      TRAIN_00003  223        1   221.4     223    25.1   233.0      61   \n4      TRAIN_00004  222        0    96.3     222    28.7   223.9      69   \n...            ...  ...      ...     ...     ...     ...     ...     ...   \n30195  TRAIN_30195  263       80   289.6     201    21.8   280.5     323   \n30196  TRAIN_30196  283       81   210.7     280    90.5   284.1     202   \n30197  TRAIN_30197   24        0   222.4      33    22.1   233.9      32   \n30198  TRAIN_30198   63        1   262.4     202    29.6   280.6     282   \n30199  TRAIN_30199  200        1   327.1     203    86.8   330.1      82   \n\n       저녁통화요금  밤통화시간  밤통화횟수  밤통화요금  상담전화건수  전화해지여부  \n0       28.92  262.9    328  32.89       2       0  \n1       32.09  322.8    209  32.32       2       0  \n2       32.82  280.8    328   8.28       0       0  \n3       23.90  203.8    234   9.36       0       0  \n4       28.08  263.1    223   2.80       8       0  \n...       ...    ...    ...    ...     ...     ...  \n30195   29.88  208.0     66   9.28       2       0  \n30196   32.80  287.8    203   6.28       2       0  \n30197   22.22  293.6     95   4.22       2       0  \n30198   28.88  280.9    207  20.88       2       1  \n30199   28.88  289.1    323   6.28       0       0  \n\n[30200 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>가입일</th>\n      <th>음성사서함이용</th>\n      <th>주간통화시간</th>\n      <th>주간통화횟수</th>\n      <th>주간통화요금</th>\n      <th>저녁통화시간</th>\n      <th>저녁통화횟수</th>\n      <th>저녁통화요금</th>\n      <th>밤통화시간</th>\n      <th>밤통화횟수</th>\n      <th>밤통화요금</th>\n      <th>상담전화건수</th>\n      <th>전화해지여부</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRAIN_00000</td>\n      <td>329</td>\n      <td>0</td>\n      <td>99.2</td>\n      <td>93</td>\n      <td>27.3</td>\n      <td>268.8</td>\n      <td>68</td>\n      <td>28.92</td>\n      <td>262.9</td>\n      <td>328</td>\n      <td>32.89</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TRAIN_00001</td>\n      <td>2</td>\n      <td>80</td>\n      <td>323.9</td>\n      <td>323</td>\n      <td>83.7</td>\n      <td>269.4</td>\n      <td>326</td>\n      <td>32.09</td>\n      <td>322.8</td>\n      <td>209</td>\n      <td>32.32</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRAIN_00002</td>\n      <td>93</td>\n      <td>28</td>\n      <td>282.4</td>\n      <td>323</td>\n      <td>34.2</td>\n      <td>207.0</td>\n      <td>322</td>\n      <td>32.82</td>\n      <td>280.8</td>\n      <td>328</td>\n      <td>8.28</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRAIN_00003</td>\n      <td>223</td>\n      <td>1</td>\n      <td>221.4</td>\n      <td>223</td>\n      <td>25.1</td>\n      <td>233.0</td>\n      <td>61</td>\n      <td>23.90</td>\n      <td>203.8</td>\n      <td>234</td>\n      <td>9.36</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TRAIN_00004</td>\n      <td>222</td>\n      <td>0</td>\n      <td>96.3</td>\n      <td>222</td>\n      <td>28.7</td>\n      <td>223.9</td>\n      <td>69</td>\n      <td>28.08</td>\n      <td>263.1</td>\n      <td>223</td>\n      <td>2.80</td>\n      <td>8</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30195</th>\n      <td>TRAIN_30195</td>\n      <td>263</td>\n      <td>80</td>\n      <td>289.6</td>\n      <td>201</td>\n      <td>21.8</td>\n      <td>280.5</td>\n      <td>323</td>\n      <td>29.88</td>\n      <td>208.0</td>\n      <td>66</td>\n      <td>9.28</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30196</th>\n      <td>TRAIN_30196</td>\n      <td>283</td>\n      <td>81</td>\n      <td>210.7</td>\n      <td>280</td>\n      <td>90.5</td>\n      <td>284.1</td>\n      <td>202</td>\n      <td>32.80</td>\n      <td>287.8</td>\n      <td>203</td>\n      <td>6.28</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30197</th>\n      <td>TRAIN_30197</td>\n      <td>24</td>\n      <td>0</td>\n      <td>222.4</td>\n      <td>33</td>\n      <td>22.1</td>\n      <td>233.9</td>\n      <td>32</td>\n      <td>22.22</td>\n      <td>293.6</td>\n      <td>95</td>\n      <td>4.22</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30198</th>\n      <td>TRAIN_30198</td>\n      <td>63</td>\n      <td>1</td>\n      <td>262.4</td>\n      <td>202</td>\n      <td>29.6</td>\n      <td>280.6</td>\n      <td>282</td>\n      <td>28.88</td>\n      <td>280.9</td>\n      <td>207</td>\n      <td>20.88</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>30199</th>\n      <td>TRAIN_30199</td>\n      <td>200</td>\n      <td>1</td>\n      <td>327.1</td>\n      <td>203</td>\n      <td>86.8</td>\n      <td>330.1</td>\n      <td>82</td>\n      <td>28.88</td>\n      <td>289.1</td>\n      <td>323</td>\n      <td>6.28</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>30200 rows × 14 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train[\"전화해지여부\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-03-30T18:54:17.636249Z","iopub.execute_input":"2023-03-30T18:54:17.636689Z","iopub.status.idle":"2023-03-30T18:54:17.645453Z","shell.execute_reply.started":"2023-03-30T18:54:17.636648Z","shell.execute_reply":"2023-03-30T18:54:17.644304Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"0    26882\n1     3318\nName: 전화해지여부, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"test=pd.read_csv(\"/kaggle/input/call-prediction/test.csv\")\ntest","metadata":{"execution":{"iopub.status.busy":"2023-03-30T18:54:17.646914Z","iopub.execute_input":"2023-03-30T18:54:17.647712Z","iopub.status.idle":"2023-03-30T18:54:17.689692Z","shell.execute_reply.started":"2023-03-30T18:54:17.647674Z","shell.execute_reply":"2023-03-30T18:54:17.688621Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"               ID  가입일  음성사서함이용  주간통화시간  주간통화횟수  주간통화요금  저녁통화시간  저녁통화횟수  \\\n0      TEST_00000  110        0   210.7     142    35.6   205.9      93   \n1      TEST_00001  283        1   268.5     100    30.3   283.0      92   \n2      TEST_00002   34        1   243.3     205    21.3   225.7     205   \n3      TEST_00003  209       86    83.0      93    24.1   270.6     220   \n4      TEST_00004  202        0   293.2      23    31.0   294.0      24   \n...           ...  ...      ...     ...     ...     ...     ...     ...   \n12938  TEST_12938  322        1   282.4     202    89.9   270.2     321   \n12939  TEST_12939  222        0   233.5     209    34.6   234.0      92   \n12940  TEST_12940   98       29   223.4     209    29.3   223.9     203   \n12941  TEST_12941   23       30   241.3     222    24.7   234.3      93   \n12942  TEST_12942   28       22   281.7     228    24.1   208.8     220   \n\n       저녁통화요금  밤통화시간  밤통화횟수  밤통화요금  상담전화건수  \n0       17.43  120.9    111   5.37       2  \n1       28.32  290.0     89   8.82       0  \n2       29.00   92.6     29   2.44       2  \n3       22.89  266.9     81   2.80       6  \n4       26.39  223.8     33   6.66       2  \n...       ...    ...    ...    ...     ...  \n12938   32.80  293.6    321   8.88       2  \n12939   23.22  224.4    202   9.63       2  \n12940   28.82  230.0    208  22.26       2  \n12941   22.23  223.1     33   9.93       0  \n12942   22.29  222.9     68   2.28       2  \n\n[12943 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>가입일</th>\n      <th>음성사서함이용</th>\n      <th>주간통화시간</th>\n      <th>주간통화횟수</th>\n      <th>주간통화요금</th>\n      <th>저녁통화시간</th>\n      <th>저녁통화횟수</th>\n      <th>저녁통화요금</th>\n      <th>밤통화시간</th>\n      <th>밤통화횟수</th>\n      <th>밤통화요금</th>\n      <th>상담전화건수</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TEST_00000</td>\n      <td>110</td>\n      <td>0</td>\n      <td>210.7</td>\n      <td>142</td>\n      <td>35.6</td>\n      <td>205.9</td>\n      <td>93</td>\n      <td>17.43</td>\n      <td>120.9</td>\n      <td>111</td>\n      <td>5.37</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TEST_00001</td>\n      <td>283</td>\n      <td>1</td>\n      <td>268.5</td>\n      <td>100</td>\n      <td>30.3</td>\n      <td>283.0</td>\n      <td>92</td>\n      <td>28.32</td>\n      <td>290.0</td>\n      <td>89</td>\n      <td>8.82</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TEST_00002</td>\n      <td>34</td>\n      <td>1</td>\n      <td>243.3</td>\n      <td>205</td>\n      <td>21.3</td>\n      <td>225.7</td>\n      <td>205</td>\n      <td>29.00</td>\n      <td>92.6</td>\n      <td>29</td>\n      <td>2.44</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TEST_00003</td>\n      <td>209</td>\n      <td>86</td>\n      <td>83.0</td>\n      <td>93</td>\n      <td>24.1</td>\n      <td>270.6</td>\n      <td>220</td>\n      <td>22.89</td>\n      <td>266.9</td>\n      <td>81</td>\n      <td>2.80</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TEST_00004</td>\n      <td>202</td>\n      <td>0</td>\n      <td>293.2</td>\n      <td>23</td>\n      <td>31.0</td>\n      <td>294.0</td>\n      <td>24</td>\n      <td>26.39</td>\n      <td>223.8</td>\n      <td>33</td>\n      <td>6.66</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12938</th>\n      <td>TEST_12938</td>\n      <td>322</td>\n      <td>1</td>\n      <td>282.4</td>\n      <td>202</td>\n      <td>89.9</td>\n      <td>270.2</td>\n      <td>321</td>\n      <td>32.80</td>\n      <td>293.6</td>\n      <td>321</td>\n      <td>8.88</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>12939</th>\n      <td>TEST_12939</td>\n      <td>222</td>\n      <td>0</td>\n      <td>233.5</td>\n      <td>209</td>\n      <td>34.6</td>\n      <td>234.0</td>\n      <td>92</td>\n      <td>23.22</td>\n      <td>224.4</td>\n      <td>202</td>\n      <td>9.63</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>12940</th>\n      <td>TEST_12940</td>\n      <td>98</td>\n      <td>29</td>\n      <td>223.4</td>\n      <td>209</td>\n      <td>29.3</td>\n      <td>223.9</td>\n      <td>203</td>\n      <td>28.82</td>\n      <td>230.0</td>\n      <td>208</td>\n      <td>22.26</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>12941</th>\n      <td>TEST_12941</td>\n      <td>23</td>\n      <td>30</td>\n      <td>241.3</td>\n      <td>222</td>\n      <td>24.7</td>\n      <td>234.3</td>\n      <td>93</td>\n      <td>22.23</td>\n      <td>223.1</td>\n      <td>33</td>\n      <td>9.93</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12942</th>\n      <td>TEST_12942</td>\n      <td>28</td>\n      <td>22</td>\n      <td>281.7</td>\n      <td>228</td>\n      <td>24.1</td>\n      <td>208.8</td>\n      <td>220</td>\n      <td>22.29</td>\n      <td>222.9</td>\n      <td>68</td>\n      <td>2.28</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>12943 rows × 13 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"alldata=pd.concat([train,test])\nalldata","metadata":{"execution":{"iopub.status.busy":"2023-03-30T18:54:17.692588Z","iopub.execute_input":"2023-03-30T18:54:17.692952Z","iopub.status.idle":"2023-03-30T18:54:17.720159Z","shell.execute_reply.started":"2023-03-30T18:54:17.692915Z","shell.execute_reply":"2023-03-30T18:54:17.719085Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                ID  가입일  음성사서함이용  주간통화시간  주간통화횟수  주간통화요금  저녁통화시간  저녁통화횟수  \\\n0      TRAIN_00000  329        0    99.2      93    27.3   268.8      68   \n1      TRAIN_00001    2       80   323.9     323    83.7   269.4     326   \n2      TRAIN_00002   93       28   282.4     323    34.2   207.0     322   \n3      TRAIN_00003  223        1   221.4     223    25.1   233.0      61   \n4      TRAIN_00004  222        0    96.3     222    28.7   223.9      69   \n...            ...  ...      ...     ...     ...     ...     ...     ...   \n12938   TEST_12938  322        1   282.4     202    89.9   270.2     321   \n12939   TEST_12939  222        0   233.5     209    34.6   234.0      92   \n12940   TEST_12940   98       29   223.4     209    29.3   223.9     203   \n12941   TEST_12941   23       30   241.3     222    24.7   234.3      93   \n12942   TEST_12942   28       22   281.7     228    24.1   208.8     220   \n\n       저녁통화요금  밤통화시간  밤통화횟수  밤통화요금  상담전화건수  전화해지여부  \n0       28.92  262.9    328  32.89       2     0.0  \n1       32.09  322.8    209  32.32       2     0.0  \n2       32.82  280.8    328   8.28       0     0.0  \n3       23.90  203.8    234   9.36       0     0.0  \n4       28.08  263.1    223   2.80       8     0.0  \n...       ...    ...    ...    ...     ...     ...  \n12938   32.80  293.6    321   8.88       2     NaN  \n12939   23.22  224.4    202   9.63       2     NaN  \n12940   28.82  230.0    208  22.26       2     NaN  \n12941   22.23  223.1     33   9.93       0     NaN  \n12942   22.29  222.9     68   2.28       2     NaN  \n\n[43143 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>가입일</th>\n      <th>음성사서함이용</th>\n      <th>주간통화시간</th>\n      <th>주간통화횟수</th>\n      <th>주간통화요금</th>\n      <th>저녁통화시간</th>\n      <th>저녁통화횟수</th>\n      <th>저녁통화요금</th>\n      <th>밤통화시간</th>\n      <th>밤통화횟수</th>\n      <th>밤통화요금</th>\n      <th>상담전화건수</th>\n      <th>전화해지여부</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRAIN_00000</td>\n      <td>329</td>\n      <td>0</td>\n      <td>99.2</td>\n      <td>93</td>\n      <td>27.3</td>\n      <td>268.8</td>\n      <td>68</td>\n      <td>28.92</td>\n      <td>262.9</td>\n      <td>328</td>\n      <td>32.89</td>\n      <td>2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TRAIN_00001</td>\n      <td>2</td>\n      <td>80</td>\n      <td>323.9</td>\n      <td>323</td>\n      <td>83.7</td>\n      <td>269.4</td>\n      <td>326</td>\n      <td>32.09</td>\n      <td>322.8</td>\n      <td>209</td>\n      <td>32.32</td>\n      <td>2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRAIN_00002</td>\n      <td>93</td>\n      <td>28</td>\n      <td>282.4</td>\n      <td>323</td>\n      <td>34.2</td>\n      <td>207.0</td>\n      <td>322</td>\n      <td>32.82</td>\n      <td>280.8</td>\n      <td>328</td>\n      <td>8.28</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRAIN_00003</td>\n      <td>223</td>\n      <td>1</td>\n      <td>221.4</td>\n      <td>223</td>\n      <td>25.1</td>\n      <td>233.0</td>\n      <td>61</td>\n      <td>23.90</td>\n      <td>203.8</td>\n      <td>234</td>\n      <td>9.36</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TRAIN_00004</td>\n      <td>222</td>\n      <td>0</td>\n      <td>96.3</td>\n      <td>222</td>\n      <td>28.7</td>\n      <td>223.9</td>\n      <td>69</td>\n      <td>28.08</td>\n      <td>263.1</td>\n      <td>223</td>\n      <td>2.80</td>\n      <td>8</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12938</th>\n      <td>TEST_12938</td>\n      <td>322</td>\n      <td>1</td>\n      <td>282.4</td>\n      <td>202</td>\n      <td>89.9</td>\n      <td>270.2</td>\n      <td>321</td>\n      <td>32.80</td>\n      <td>293.6</td>\n      <td>321</td>\n      <td>8.88</td>\n      <td>2</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12939</th>\n      <td>TEST_12939</td>\n      <td>222</td>\n      <td>0</td>\n      <td>233.5</td>\n      <td>209</td>\n      <td>34.6</td>\n      <td>234.0</td>\n      <td>92</td>\n      <td>23.22</td>\n      <td>224.4</td>\n      <td>202</td>\n      <td>9.63</td>\n      <td>2</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12940</th>\n      <td>TEST_12940</td>\n      <td>98</td>\n      <td>29</td>\n      <td>223.4</td>\n      <td>209</td>\n      <td>29.3</td>\n      <td>223.9</td>\n      <td>203</td>\n      <td>28.82</td>\n      <td>230.0</td>\n      <td>208</td>\n      <td>22.26</td>\n      <td>2</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12941</th>\n      <td>TEST_12941</td>\n      <td>23</td>\n      <td>30</td>\n      <td>241.3</td>\n      <td>222</td>\n      <td>24.7</td>\n      <td>234.3</td>\n      <td>93</td>\n      <td>22.23</td>\n      <td>223.1</td>\n      <td>33</td>\n      <td>9.93</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12942</th>\n      <td>TEST_12942</td>\n      <td>28</td>\n      <td>22</td>\n      <td>281.7</td>\n      <td>228</td>\n      <td>24.1</td>\n      <td>208.8</td>\n      <td>220</td>\n      <td>22.29</td>\n      <td>222.9</td>\n      <td>68</td>\n      <td>2.28</td>\n      <td>2</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>43143 rows × 14 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# alldata[\"상담전화여부\"] = alldata[\"상담전화건수\"].apply(lambda x: 0 if x == 0 else 2 if x > 2 else 1)\n# a=alldata.groupby(\"상담전화여부\")[\"음성사서함이용\"].agg([\"mean\",\"count\",\"std\",\"max\"])\n# alldata= pd.merge(alldata,a,on=\"상담전화여부\",how=\"left\")","metadata":{"execution":{"iopub.status.busy":"2023-03-30T18:54:17.721553Z","iopub.execute_input":"2023-03-30T18:54:17.721882Z","iopub.status.idle":"2023-03-30T18:54:17.728793Z","shell.execute_reply.started":"2023-03-30T18:54:17.721847Z","shell.execute_reply":"2023-03-30T18:54:17.727732Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"alldata[\"평균음성사서함\"] = alldata[\"음성사서함이용\"] / alldata[\"가입일\"]\nalldata.groupby(\"전화해지여부\")[\"평균음성사서함\"].mean()\nalldata","metadata":{"execution":{"iopub.status.busy":"2023-03-30T18:54:17.730534Z","iopub.execute_input":"2023-03-30T18:54:17.731005Z","iopub.status.idle":"2023-03-30T18:54:17.758565Z","shell.execute_reply.started":"2023-03-30T18:54:17.730966Z","shell.execute_reply":"2023-03-30T18:54:17.757531Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                ID  가입일  음성사서함이용  주간통화시간  주간통화횟수  주간통화요금  저녁통화시간  저녁통화횟수  \\\n0      TRAIN_00000  329        0    99.2      93    27.3   268.8      68   \n1      TRAIN_00001    2       80   323.9     323    83.7   269.4     326   \n2      TRAIN_00002   93       28   282.4     323    34.2   207.0     322   \n3      TRAIN_00003  223        1   221.4     223    25.1   233.0      61   \n4      TRAIN_00004  222        0    96.3     222    28.7   223.9      69   \n...            ...  ...      ...     ...     ...     ...     ...     ...   \n12938   TEST_12938  322        1   282.4     202    89.9   270.2     321   \n12939   TEST_12939  222        0   233.5     209    34.6   234.0      92   \n12940   TEST_12940   98       29   223.4     209    29.3   223.9     203   \n12941   TEST_12941   23       30   241.3     222    24.7   234.3      93   \n12942   TEST_12942   28       22   281.7     228    24.1   208.8     220   \n\n       저녁통화요금  밤통화시간  밤통화횟수  밤통화요금  상담전화건수  전화해지여부    평균음성사서함  \n0       28.92  262.9    328  32.89       2     0.0   0.000000  \n1       32.09  322.8    209  32.32       2     0.0  40.000000  \n2       32.82  280.8    328   8.28       0     0.0   0.301075  \n3       23.90  203.8    234   9.36       0     0.0   0.004484  \n4       28.08  263.1    223   2.80       8     0.0   0.000000  \n...       ...    ...    ...    ...     ...     ...        ...  \n12938   32.80  293.6    321   8.88       2     NaN   0.003106  \n12939   23.22  224.4    202   9.63       2     NaN   0.000000  \n12940   28.82  230.0    208  22.26       2     NaN   0.295918  \n12941   22.23  223.1     33   9.93       0     NaN   1.304348  \n12942   22.29  222.9     68   2.28       2     NaN   0.785714  \n\n[43143 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>가입일</th>\n      <th>음성사서함이용</th>\n      <th>주간통화시간</th>\n      <th>주간통화횟수</th>\n      <th>주간통화요금</th>\n      <th>저녁통화시간</th>\n      <th>저녁통화횟수</th>\n      <th>저녁통화요금</th>\n      <th>밤통화시간</th>\n      <th>밤통화횟수</th>\n      <th>밤통화요금</th>\n      <th>상담전화건수</th>\n      <th>전화해지여부</th>\n      <th>평균음성사서함</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRAIN_00000</td>\n      <td>329</td>\n      <td>0</td>\n      <td>99.2</td>\n      <td>93</td>\n      <td>27.3</td>\n      <td>268.8</td>\n      <td>68</td>\n      <td>28.92</td>\n      <td>262.9</td>\n      <td>328</td>\n      <td>32.89</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TRAIN_00001</td>\n      <td>2</td>\n      <td>80</td>\n      <td>323.9</td>\n      <td>323</td>\n      <td>83.7</td>\n      <td>269.4</td>\n      <td>326</td>\n      <td>32.09</td>\n      <td>322.8</td>\n      <td>209</td>\n      <td>32.32</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>40.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRAIN_00002</td>\n      <td>93</td>\n      <td>28</td>\n      <td>282.4</td>\n      <td>323</td>\n      <td>34.2</td>\n      <td>207.0</td>\n      <td>322</td>\n      <td>32.82</td>\n      <td>280.8</td>\n      <td>328</td>\n      <td>8.28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.301075</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRAIN_00003</td>\n      <td>223</td>\n      <td>1</td>\n      <td>221.4</td>\n      <td>223</td>\n      <td>25.1</td>\n      <td>233.0</td>\n      <td>61</td>\n      <td>23.90</td>\n      <td>203.8</td>\n      <td>234</td>\n      <td>9.36</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.004484</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TRAIN_00004</td>\n      <td>222</td>\n      <td>0</td>\n      <td>96.3</td>\n      <td>222</td>\n      <td>28.7</td>\n      <td>223.9</td>\n      <td>69</td>\n      <td>28.08</td>\n      <td>263.1</td>\n      <td>223</td>\n      <td>2.80</td>\n      <td>8</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12938</th>\n      <td>TEST_12938</td>\n      <td>322</td>\n      <td>1</td>\n      <td>282.4</td>\n      <td>202</td>\n      <td>89.9</td>\n      <td>270.2</td>\n      <td>321</td>\n      <td>32.80</td>\n      <td>293.6</td>\n      <td>321</td>\n      <td>8.88</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>0.003106</td>\n    </tr>\n    <tr>\n      <th>12939</th>\n      <td>TEST_12939</td>\n      <td>222</td>\n      <td>0</td>\n      <td>233.5</td>\n      <td>209</td>\n      <td>34.6</td>\n      <td>234.0</td>\n      <td>92</td>\n      <td>23.22</td>\n      <td>224.4</td>\n      <td>202</td>\n      <td>9.63</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>12940</th>\n      <td>TEST_12940</td>\n      <td>98</td>\n      <td>29</td>\n      <td>223.4</td>\n      <td>209</td>\n      <td>29.3</td>\n      <td>223.9</td>\n      <td>203</td>\n      <td>28.82</td>\n      <td>230.0</td>\n      <td>208</td>\n      <td>22.26</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>0.295918</td>\n    </tr>\n    <tr>\n      <th>12941</th>\n      <td>TEST_12941</td>\n      <td>23</td>\n      <td>30</td>\n      <td>241.3</td>\n      <td>222</td>\n      <td>24.7</td>\n      <td>234.3</td>\n      <td>93</td>\n      <td>22.23</td>\n      <td>223.1</td>\n      <td>33</td>\n      <td>9.93</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>1.304348</td>\n    </tr>\n    <tr>\n      <th>12942</th>\n      <td>TEST_12942</td>\n      <td>28</td>\n      <td>22</td>\n      <td>281.7</td>\n      <td>228</td>\n      <td>24.1</td>\n      <td>208.8</td>\n      <td>220</td>\n      <td>22.29</td>\n      <td>222.9</td>\n      <td>68</td>\n      <td>2.28</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>0.785714</td>\n    </tr>\n  </tbody>\n</table>\n<p>43143 rows × 15 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"alldata[\"통화요금\"] = alldata[\"주간통화요금\"] + alldata[\"저녁통화요금\"]","metadata":{"execution":{"iopub.status.busy":"2023-03-30T18:54:17.760186Z","iopub.execute_input":"2023-03-30T18:54:17.760530Z","iopub.status.idle":"2023-03-30T18:54:17.768165Z","shell.execute_reply.started":"2023-03-30T18:54:17.760496Z","shell.execute_reply":"2023-03-30T18:54:17.767121Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(16,8))\nsns.countplot(data=alldata,x=\"통화요금\",y=\"전화해지여부\",showfliers=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:44:30.547440Z","iopub.status.idle":"2023-03-30T19:44:30.548127Z","shell.execute_reply.started":"2023-03-30T19:44:30.547840Z","shell.execute_reply":"2023-03-30T19:44:30.547874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alldata[\"상담전화여부\"] = alldata[\"상담전화건수\"].apply(lambda x: 0 if x==0 else 1)\nalldata[\"음성사서함이용여부\"] = alldata[\"음성사서함이용\"].apply(lambda x: 1 if x>0 else 0)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T18:54:17.769449Z","iopub.execute_input":"2023-03-30T18:54:17.770527Z","iopub.status.idle":"2023-03-30T18:54:17.807817Z","shell.execute_reply.started":"2023-03-30T18:54:17.770489Z","shell.execute_reply":"2023-03-30T18:54:17.806975Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,8))\nsns.boxplot(data=alldata,x=\"전화해지여부\",y=\"음성사서함이용\",showfliers=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:52:44.455824Z","iopub.execute_input":"2023-03-30T19:52:44.456199Z","iopub.status.idle":"2023-03-30T19:52:44.737111Z","shell.execute_reply.started":"2023-03-30T19:52:44.456167Z","shell.execute_reply":"2023-03-30T19:52:44.736161Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:xlabel='전화해지여부', ylabel='음성사서함이용'>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 51204 (\\N{HANGUL SYLLABLE JEON}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 54868 (\\N{HANGUL SYLLABLE HWA}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 54644 (\\N{HANGUL SYLLABLE HAE}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 51648 (\\N{HANGUL SYLLABLE JI}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 50668 (\\N{HANGUL SYLLABLE YEO}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 48512 (\\N{HANGUL SYLLABLE BU}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 51020 (\\N{HANGUL SYLLABLE EUM}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 49457 (\\N{HANGUL SYLLABLE SEONG}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 49324 (\\N{HANGUL SYLLABLE SA}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 49436 (\\N{HANGUL SYLLABLE SEO}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 54632 (\\N{HANGUL SYLLABLE HAM}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 50857 (\\N{HANGUL SYLLABLE YONG}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51204 (\\N{HANGUL SYLLABLE JEON}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54868 (\\N{HANGUL SYLLABLE HWA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54644 (\\N{HANGUL SYLLABLE HAE}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51648 (\\N{HANGUL SYLLABLE JI}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50668 (\\N{HANGUL SYLLABLE YEO}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48512 (\\N{HANGUL SYLLABLE BU}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51020 (\\N{HANGUL SYLLABLE EUM}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49457 (\\N{HANGUL SYLLABLE SEONG}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49324 (\\N{HANGUL SYLLABLE SA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49436 (\\N{HANGUL SYLLABLE SEO}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54632 (\\N{HANGUL SYLLABLE HAM}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50857 (\\N{HANGUL SYLLABLE YONG}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1600x800 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(16,8))\nsns.boxplot(data=alldata,x=\"전화해지여부\",y=\"상담전화건수\",showfliers=False,)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T18:59:03.367783Z","iopub.execute_input":"2023-03-30T18:59:03.368186Z","iopub.status.idle":"2023-03-30T18:59:03.623504Z","shell.execute_reply.started":"2023-03-30T18:59:03.368150Z","shell.execute_reply":"2023-03-30T18:59:03.622555Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:xlabel='전화해지여부', ylabel='상담전화건수'>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 51204 (\\N{HANGUL SYLLABLE JEON}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 54868 (\\N{HANGUL SYLLABLE HWA}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 54644 (\\N{HANGUL SYLLABLE HAE}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 51648 (\\N{HANGUL SYLLABLE JI}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 50668 (\\N{HANGUL SYLLABLE YEO}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 48512 (\\N{HANGUL SYLLABLE BU}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 49345 (\\N{HANGUL SYLLABLE SANG}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 45812 (\\N{HANGUL SYLLABLE DAM}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 44148 (\\N{HANGUL SYLLABLE GEON}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/events.py:89: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from current font.\n  func(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51204 (\\N{HANGUL SYLLABLE JEON}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54868 (\\N{HANGUL SYLLABLE HWA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54644 (\\N{HANGUL SYLLABLE HAE}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51648 (\\N{HANGUL SYLLABLE JI}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50668 (\\N{HANGUL SYLLABLE YEO}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48512 (\\N{HANGUL SYLLABLE BU}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49345 (\\N{HANGUL SYLLABLE SANG}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 45812 (\\N{HANGUL SYLLABLE DAM}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 44148 (\\N{HANGUL SYLLABLE GEON}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1600x800 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"alldata[\"가입일_3\"] = alldata[\"가입일\"].apply(lambda x: 0 if x <= 7 else 2 if x>30 else 1)\na = alldata.groupby(\"가입일_3\")[\"주간통화요금\", \"밤통화시간\", \"밤통화횟수\",].agg([\"mean\", \"sum\", \"count\", \"std\", \"max\"])\nalldata = pd.merge(alldata, a, how = \"left\", on = \"가입일_3\")","metadata":{"execution":{"iopub.status.busy":"2023-03-30T18:54:18.442710Z","iopub.status.idle":"2023-03-30T18:54:18.443083Z","shell.execute_reply.started":"2023-03-30T18:54:18.442903Z","shell.execute_reply":"2023-03-30T18:54:18.442921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alldata[\"상담전화카테고리\"] = alldata[\"상담전화건수\"].apply(lambda x: 0 if x==0 else 1 if ((x==1)) else 2 if ((x==2)|(x==3)) else 3 )#|(x==4)\nb = alldata.groupby(\"상담전화카테고리\")[\"저녁통화시간\"].agg([\"mean\", \"sum\", \"count\", \"std\", \"max\",])\nalldata = pd.merge(alldata, b, how = \"left\", on = \"상담전화카테고리\")","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:35:05.683290Z","iopub.execute_input":"2023-03-26T20:35:05.683950Z","iopub.status.idle":"2023-03-26T20:35:05.727631Z","shell.execute_reply.started":"2023-03-26T20:35:05.683891Z","shell.execute_reply":"2023-03-26T20:35:05.726695Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"code","source":"#내가 추가한 칼럼\n# b = alldata.groupby(\"\")[\"상담전화여부\"].mean()\n# alldata = pd.merge(alldata,b,on=\"주간통화요금\",how=\"left\")\n# alldata\n#best_score 가 아주살짝 좋아짐.\n# pd.options.display.max_columns=999\n# alldata[\"요금\"] = alldata[\"통화요금\"] / alldata[\"가입일\"]\n# alldata.groupby(\"음성사서함이용\")[\"요금\"].mean()\n# alldata","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:35:05.729183Z","iopub.execute_input":"2023-03-26T20:35:05.729588Z","iopub.status.idle":"2023-03-26T20:35:05.735475Z","shell.execute_reply.started":"2023-03-26T20:35:05.729552Z","shell.execute_reply":"2023-03-26T20:35:05.733224Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"alldata2=alldata.drop(columns=[\"ID\",\"전화해지여부\",\"가입일_3\",\"상담전화카테고리\"])\nalldata2","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:35:05.737031Z","iopub.execute_input":"2023-03-26T20:35:05.738376Z","iopub.status.idle":"2023-03-26T20:35:05.796465Z","shell.execute_reply.started":"2023-03-26T20:35:05.738338Z","shell.execute_reply":"2023-03-26T20:35:05.795345Z"},"trusted":true},"execution_count":148,"outputs":[{"execution_count":148,"output_type":"execute_result","data":{"text/plain":"       가입일  음성사서함이용  주간통화시간  주간통화횟수  주간통화요금  저녁통화시간  저녁통화횟수  저녁통화요금  밤통화시간  \\\n0      329        0    99.2      93    27.3   268.8      68   28.92  262.9   \n1        2       80   323.9     323    83.7   269.4     326   32.09  322.8   \n2       93       28   282.4     323    34.2   207.0     322   32.82  280.8   \n3      223        1   221.4     223    25.1   233.0      61   23.90  203.8   \n4      222        0    96.3     222    28.7   223.9      69   28.08  263.1   \n...    ...      ...     ...     ...     ...     ...     ...     ...    ...   \n43138  322        1   282.4     202    89.9   270.2     321   32.80  293.6   \n43139  222        0   233.5     209    34.6   234.0      92   23.22  224.4   \n43140   98       29   223.4     209    29.3   223.9     203   28.82  230.0   \n43141   23       30   241.3     222    24.7   234.3      93   22.23  223.1   \n43142   28       22   281.7     228    24.1   208.8     220   22.29  222.9   \n\n       밤통화횟수  밤통화요금  상담전화건수    평균음성사서함    통화요금  상담전화여부  음성사서함이용여부  \\\n0        328  32.89       2   0.000000   56.22       1          0   \n1        209  32.32       2  40.000000  115.79       1          1   \n2        328   8.28       0   0.301075   67.02       0          1   \n3        234   9.36       0   0.004484   49.00       0          1   \n4        223   2.80       8   0.000000   56.78       1          0   \n...      ...    ...     ...        ...     ...     ...        ...   \n43138    321   8.88       2   0.003106  122.70       1          1   \n43139    202   9.63       2   0.000000   57.82       1          0   \n43140    208  22.26       2   0.295918   58.12       1          1   \n43141     33   9.93       0   1.304348   46.93       0          1   \n43142     68   2.28       2   0.785714   46.39       1          1   \n\n       (주간통화요금, mean)  (주간통화요금, sum)  (주간통화요금, count)  (주간통화요금, std)  \\\n0           41.157451      1561019.8            37928      23.043226   \n1           44.200966         9149.6              207      24.477860   \n2           41.157451      1561019.8            37928      23.043226   \n3           41.157451      1561019.8            37928      23.043226   \n4           41.157451      1561019.8            37928      23.043226   \n...               ...            ...              ...            ...   \n43138       41.157451      1561019.8            37928      23.043226   \n43139       41.157451      1561019.8            37928      23.043226   \n43140       41.157451      1561019.8            37928      23.043226   \n43141       39.934804       199993.5             5008      23.808004   \n43142       39.934804       199993.5             5008      23.808004   \n\n       (주간통화요금, max)  (밤통화시간, mean)  (밤통화시간, sum)  (밤통화시간, count)  \\\n0              118.6     266.268580    10099034.7           37928   \n1               90.8     243.194203       50341.2             207   \n2              118.6     266.268580    10099034.7           37928   \n3              118.6     266.268580    10099034.7           37928   \n4              118.6     266.268580    10099034.7           37928   \n...              ...            ...           ...             ...   \n43138          118.6     266.268580    10099034.7           37928   \n43139          118.6     266.268580    10099034.7           37928   \n43140          118.6     266.268580    10099034.7           37928   \n43141          116.8     244.428474     1224097.8            5008   \n43142          116.8     244.428474     1224097.8            5008   \n\n       (밤통화시간, std)  (밤통화시간, max)  (밤통화횟수, mean)  (밤통화횟수, sum)  \\\n0         76.476869         481.8     158.606518       6015628   \n1         82.156311         477.8     135.908213         28133   \n2         76.476869         481.8     158.606518       6015628   \n3         76.476869         481.8     158.606518       6015628   \n4         76.476869         481.8     158.606518       6015628   \n...             ...           ...            ...           ...   \n43138     76.476869         481.8     158.606518       6015628   \n43139     76.476869         481.8     158.606518       6015628   \n43140     76.476869         481.8     158.606518       6015628   \n43141     47.994978         478.8     147.779553        740080   \n43142     47.994978         478.8     147.779553        740080   \n\n       (밤통화횟수, count)  (밤통화횟수, std)  (밤통화횟수, max)        mean        sum  \\\n0               37928    103.512500           490  248.086617  6269396.9   \n1                 207    105.480023           449  248.086617  6269396.9   \n2               37928    103.512500           490  263.000022  2363581.2   \n3               37928    103.512500           490  263.000022  2363581.2   \n4               37928    103.512500           490  323.123871  2489346.3   \n...               ...           ...           ...         ...        ...   \n43138           37928    103.512500           490  248.086617  6269396.9   \n43139           37928    103.512500           490  248.086617  6269396.9   \n43140           37928    103.512500           490  248.086617  6269396.9   \n43141            5008     85.841581           481  263.000022  2363581.2   \n43142            5008     85.841581           481  248.086617  6269396.9   \n\n       count         std    max  \n0      25271   48.889982  451.4  \n1      25271   48.889982  451.4  \n2       8987   75.816204  481.4  \n3       8987   75.816204  481.4  \n4       7704  105.727774  481.6  \n...      ...         ...    ...  \n43138  25271   48.889982  451.4  \n43139  25271   48.889982  451.4  \n43140  25271   48.889982  451.4  \n43141   8987   75.816204  481.4  \n43142  25271   48.889982  451.4  \n\n[43143 rows x 36 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>가입일</th>\n      <th>음성사서함이용</th>\n      <th>주간통화시간</th>\n      <th>주간통화횟수</th>\n      <th>주간통화요금</th>\n      <th>저녁통화시간</th>\n      <th>저녁통화횟수</th>\n      <th>저녁통화요금</th>\n      <th>밤통화시간</th>\n      <th>밤통화횟수</th>\n      <th>밤통화요금</th>\n      <th>상담전화건수</th>\n      <th>평균음성사서함</th>\n      <th>통화요금</th>\n      <th>상담전화여부</th>\n      <th>음성사서함이용여부</th>\n      <th>(주간통화요금, mean)</th>\n      <th>(주간통화요금, sum)</th>\n      <th>(주간통화요금, count)</th>\n      <th>(주간통화요금, std)</th>\n      <th>(주간통화요금, max)</th>\n      <th>(밤통화시간, mean)</th>\n      <th>(밤통화시간, sum)</th>\n      <th>(밤통화시간, count)</th>\n      <th>(밤통화시간, std)</th>\n      <th>(밤통화시간, max)</th>\n      <th>(밤통화횟수, mean)</th>\n      <th>(밤통화횟수, sum)</th>\n      <th>(밤통화횟수, count)</th>\n      <th>(밤통화횟수, std)</th>\n      <th>(밤통화횟수, max)</th>\n      <th>mean</th>\n      <th>sum</th>\n      <th>count</th>\n      <th>std</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>329</td>\n      <td>0</td>\n      <td>99.2</td>\n      <td>93</td>\n      <td>27.3</td>\n      <td>268.8</td>\n      <td>68</td>\n      <td>28.92</td>\n      <td>262.9</td>\n      <td>328</td>\n      <td>32.89</td>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>56.22</td>\n      <td>1</td>\n      <td>0</td>\n      <td>41.157451</td>\n      <td>1561019.8</td>\n      <td>37928</td>\n      <td>23.043226</td>\n      <td>118.6</td>\n      <td>266.268580</td>\n      <td>10099034.7</td>\n      <td>37928</td>\n      <td>76.476869</td>\n      <td>481.8</td>\n      <td>158.606518</td>\n      <td>6015628</td>\n      <td>37928</td>\n      <td>103.512500</td>\n      <td>490</td>\n      <td>248.086617</td>\n      <td>6269396.9</td>\n      <td>25271</td>\n      <td>48.889982</td>\n      <td>451.4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>80</td>\n      <td>323.9</td>\n      <td>323</td>\n      <td>83.7</td>\n      <td>269.4</td>\n      <td>326</td>\n      <td>32.09</td>\n      <td>322.8</td>\n      <td>209</td>\n      <td>32.32</td>\n      <td>2</td>\n      <td>40.000000</td>\n      <td>115.79</td>\n      <td>1</td>\n      <td>1</td>\n      <td>44.200966</td>\n      <td>9149.6</td>\n      <td>207</td>\n      <td>24.477860</td>\n      <td>90.8</td>\n      <td>243.194203</td>\n      <td>50341.2</td>\n      <td>207</td>\n      <td>82.156311</td>\n      <td>477.8</td>\n      <td>135.908213</td>\n      <td>28133</td>\n      <td>207</td>\n      <td>105.480023</td>\n      <td>449</td>\n      <td>248.086617</td>\n      <td>6269396.9</td>\n      <td>25271</td>\n      <td>48.889982</td>\n      <td>451.4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>93</td>\n      <td>28</td>\n      <td>282.4</td>\n      <td>323</td>\n      <td>34.2</td>\n      <td>207.0</td>\n      <td>322</td>\n      <td>32.82</td>\n      <td>280.8</td>\n      <td>328</td>\n      <td>8.28</td>\n      <td>0</td>\n      <td>0.301075</td>\n      <td>67.02</td>\n      <td>0</td>\n      <td>1</td>\n      <td>41.157451</td>\n      <td>1561019.8</td>\n      <td>37928</td>\n      <td>23.043226</td>\n      <td>118.6</td>\n      <td>266.268580</td>\n      <td>10099034.7</td>\n      <td>37928</td>\n      <td>76.476869</td>\n      <td>481.8</td>\n      <td>158.606518</td>\n      <td>6015628</td>\n      <td>37928</td>\n      <td>103.512500</td>\n      <td>490</td>\n      <td>263.000022</td>\n      <td>2363581.2</td>\n      <td>8987</td>\n      <td>75.816204</td>\n      <td>481.4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>223</td>\n      <td>1</td>\n      <td>221.4</td>\n      <td>223</td>\n      <td>25.1</td>\n      <td>233.0</td>\n      <td>61</td>\n      <td>23.90</td>\n      <td>203.8</td>\n      <td>234</td>\n      <td>9.36</td>\n      <td>0</td>\n      <td>0.004484</td>\n      <td>49.00</td>\n      <td>0</td>\n      <td>1</td>\n      <td>41.157451</td>\n      <td>1561019.8</td>\n      <td>37928</td>\n      <td>23.043226</td>\n      <td>118.6</td>\n      <td>266.268580</td>\n      <td>10099034.7</td>\n      <td>37928</td>\n      <td>76.476869</td>\n      <td>481.8</td>\n      <td>158.606518</td>\n      <td>6015628</td>\n      <td>37928</td>\n      <td>103.512500</td>\n      <td>490</td>\n      <td>263.000022</td>\n      <td>2363581.2</td>\n      <td>8987</td>\n      <td>75.816204</td>\n      <td>481.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>222</td>\n      <td>0</td>\n      <td>96.3</td>\n      <td>222</td>\n      <td>28.7</td>\n      <td>223.9</td>\n      <td>69</td>\n      <td>28.08</td>\n      <td>263.1</td>\n      <td>223</td>\n      <td>2.80</td>\n      <td>8</td>\n      <td>0.000000</td>\n      <td>56.78</td>\n      <td>1</td>\n      <td>0</td>\n      <td>41.157451</td>\n      <td>1561019.8</td>\n      <td>37928</td>\n      <td>23.043226</td>\n      <td>118.6</td>\n      <td>266.268580</td>\n      <td>10099034.7</td>\n      <td>37928</td>\n      <td>76.476869</td>\n      <td>481.8</td>\n      <td>158.606518</td>\n      <td>6015628</td>\n      <td>37928</td>\n      <td>103.512500</td>\n      <td>490</td>\n      <td>323.123871</td>\n      <td>2489346.3</td>\n      <td>7704</td>\n      <td>105.727774</td>\n      <td>481.6</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>43138</th>\n      <td>322</td>\n      <td>1</td>\n      <td>282.4</td>\n      <td>202</td>\n      <td>89.9</td>\n      <td>270.2</td>\n      <td>321</td>\n      <td>32.80</td>\n      <td>293.6</td>\n      <td>321</td>\n      <td>8.88</td>\n      <td>2</td>\n      <td>0.003106</td>\n      <td>122.70</td>\n      <td>1</td>\n      <td>1</td>\n      <td>41.157451</td>\n      <td>1561019.8</td>\n      <td>37928</td>\n      <td>23.043226</td>\n      <td>118.6</td>\n      <td>266.268580</td>\n      <td>10099034.7</td>\n      <td>37928</td>\n      <td>76.476869</td>\n      <td>481.8</td>\n      <td>158.606518</td>\n      <td>6015628</td>\n      <td>37928</td>\n      <td>103.512500</td>\n      <td>490</td>\n      <td>248.086617</td>\n      <td>6269396.9</td>\n      <td>25271</td>\n      <td>48.889982</td>\n      <td>451.4</td>\n    </tr>\n    <tr>\n      <th>43139</th>\n      <td>222</td>\n      <td>0</td>\n      <td>233.5</td>\n      <td>209</td>\n      <td>34.6</td>\n      <td>234.0</td>\n      <td>92</td>\n      <td>23.22</td>\n      <td>224.4</td>\n      <td>202</td>\n      <td>9.63</td>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>57.82</td>\n      <td>1</td>\n      <td>0</td>\n      <td>41.157451</td>\n      <td>1561019.8</td>\n      <td>37928</td>\n      <td>23.043226</td>\n      <td>118.6</td>\n      <td>266.268580</td>\n      <td>10099034.7</td>\n      <td>37928</td>\n      <td>76.476869</td>\n      <td>481.8</td>\n      <td>158.606518</td>\n      <td>6015628</td>\n      <td>37928</td>\n      <td>103.512500</td>\n      <td>490</td>\n      <td>248.086617</td>\n      <td>6269396.9</td>\n      <td>25271</td>\n      <td>48.889982</td>\n      <td>451.4</td>\n    </tr>\n    <tr>\n      <th>43140</th>\n      <td>98</td>\n      <td>29</td>\n      <td>223.4</td>\n      <td>209</td>\n      <td>29.3</td>\n      <td>223.9</td>\n      <td>203</td>\n      <td>28.82</td>\n      <td>230.0</td>\n      <td>208</td>\n      <td>22.26</td>\n      <td>2</td>\n      <td>0.295918</td>\n      <td>58.12</td>\n      <td>1</td>\n      <td>1</td>\n      <td>41.157451</td>\n      <td>1561019.8</td>\n      <td>37928</td>\n      <td>23.043226</td>\n      <td>118.6</td>\n      <td>266.268580</td>\n      <td>10099034.7</td>\n      <td>37928</td>\n      <td>76.476869</td>\n      <td>481.8</td>\n      <td>158.606518</td>\n      <td>6015628</td>\n      <td>37928</td>\n      <td>103.512500</td>\n      <td>490</td>\n      <td>248.086617</td>\n      <td>6269396.9</td>\n      <td>25271</td>\n      <td>48.889982</td>\n      <td>451.4</td>\n    </tr>\n    <tr>\n      <th>43141</th>\n      <td>23</td>\n      <td>30</td>\n      <td>241.3</td>\n      <td>222</td>\n      <td>24.7</td>\n      <td>234.3</td>\n      <td>93</td>\n      <td>22.23</td>\n      <td>223.1</td>\n      <td>33</td>\n      <td>9.93</td>\n      <td>0</td>\n      <td>1.304348</td>\n      <td>46.93</td>\n      <td>0</td>\n      <td>1</td>\n      <td>39.934804</td>\n      <td>199993.5</td>\n      <td>5008</td>\n      <td>23.808004</td>\n      <td>116.8</td>\n      <td>244.428474</td>\n      <td>1224097.8</td>\n      <td>5008</td>\n      <td>47.994978</td>\n      <td>478.8</td>\n      <td>147.779553</td>\n      <td>740080</td>\n      <td>5008</td>\n      <td>85.841581</td>\n      <td>481</td>\n      <td>263.000022</td>\n      <td>2363581.2</td>\n      <td>8987</td>\n      <td>75.816204</td>\n      <td>481.4</td>\n    </tr>\n    <tr>\n      <th>43142</th>\n      <td>28</td>\n      <td>22</td>\n      <td>281.7</td>\n      <td>228</td>\n      <td>24.1</td>\n      <td>208.8</td>\n      <td>220</td>\n      <td>22.29</td>\n      <td>222.9</td>\n      <td>68</td>\n      <td>2.28</td>\n      <td>2</td>\n      <td>0.785714</td>\n      <td>46.39</td>\n      <td>1</td>\n      <td>1</td>\n      <td>39.934804</td>\n      <td>199993.5</td>\n      <td>5008</td>\n      <td>23.808004</td>\n      <td>116.8</td>\n      <td>244.428474</td>\n      <td>1224097.8</td>\n      <td>5008</td>\n      <td>47.994978</td>\n      <td>478.8</td>\n      <td>147.779553</td>\n      <td>740080</td>\n      <td>5008</td>\n      <td>85.841581</td>\n      <td>481</td>\n      <td>248.086617</td>\n      <td>6269396.9</td>\n      <td>25271</td>\n      <td>48.889982</td>\n      <td>451.4</td>\n    </tr>\n  </tbody>\n</table>\n<p>43143 rows × 36 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train2=alldata2[:len(train)]\ntest2=alldata2[len(train):]","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:35:05.797989Z","iopub.execute_input":"2023-03-26T20:35:05.798435Z","iopub.status.idle":"2023-03-26T20:35:05.804153Z","shell.execute_reply.started":"2023-03-26T20:35:05.798396Z","shell.execute_reply":"2023-03-26T20:35:05.802983Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"#내부 교차검증 최적화 ㅣlgbm extra tree , deep learning\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import StratifiedKFold\ncbc = CatBoostClassifier(verbose = 100, learning_rate=0.05, iterations=10000)#결과 너무 자주나오는거 귀찮으니깐\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=62)#10등분이 조금 더 좋음 이 대회에서\nresult = 0 #여기다 가중치 담아줄거다\nbest_score=0 #점수 저장후 평균 점수 나타낼거임\n\nfor train_index,valid_index in skf.split(train2,train[\"전화해지여부\"]):\n    x_train= train2.iloc[train_index]\n    x_valid= train2.iloc[valid_index]\n    y_train= train[\"전화해지여부\"].iloc[train_index]\n    y_valid= train[\"전화해지여부\"].iloc[valid_index]\n    cbc.fit(x_train,y_train,eval_set=(x_valid,y_valid),early_stopping_rounds=200)\n    result+= cbc.predict_proba(test2) / 10\n    best_score+= cbc.best_score_[\"validation\"][\"Logloss\"] / 10 #이중딕셔너리로 되어있어서 껍데이 뜯어야함, #회기면 rmse,\n    \n    \n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:35:05.806190Z","iopub.execute_input":"2023-03-26T20:35:05.806692Z","iopub.status.idle":"2023-03-26T20:44:01.597309Z","shell.execute_reply.started":"2023-03-26T20:35:05.806656Z","shell.execute_reply":"2023-03-26T20:44:01.596266Z"},"trusted":true},"execution_count":150,"outputs":[{"name":"stdout","text":"0:\tlearn: 0.6529590\ttest: 0.6531441\tbest: 0.6531441 (0)\ttotal: 11ms\tremaining: 1m 49s\n100:\tlearn: 0.3051942\ttest: 0.3178615\tbest: 0.3178615 (100)\ttotal: 1.21s\tremaining: 1m 58s\n200:\tlearn: 0.2796704\ttest: 0.3005953\tbest: 0.3005953 (200)\ttotal: 2.86s\tremaining: 2m 19s\n300:\tlearn: 0.2568581\ttest: 0.2852411\tbest: 0.2852411 (300)\ttotal: 4.13s\tremaining: 2m 12s\n400:\tlearn: 0.2369302\ttest: 0.2731060\tbest: 0.2731060 (400)\ttotal: 5.4s\tremaining: 2m 9s\n500:\tlearn: 0.2197409\ttest: 0.2635158\tbest: 0.2635158 (500)\ttotal: 6.7s\tremaining: 2m 7s\n600:\tlearn: 0.2056804\ttest: 0.2560409\tbest: 0.2560409 (600)\ttotal: 7.97s\tremaining: 2m 4s\n700:\tlearn: 0.1927480\ttest: 0.2487205\tbest: 0.2487205 (700)\ttotal: 9.25s\tremaining: 2m 2s\n800:\tlearn: 0.1818005\ttest: 0.2435379\tbest: 0.2435379 (800)\ttotal: 10.5s\tremaining: 2m\n900:\tlearn: 0.1721328\ttest: 0.2383945\tbest: 0.2383945 (900)\ttotal: 11.9s\tremaining: 1m 59s\n1000:\tlearn: 0.1632582\ttest: 0.2342739\tbest: 0.2342739 (1000)\ttotal: 13.6s\tremaining: 2m 1s\n1100:\tlearn: 0.1550677\ttest: 0.2305556\tbest: 0.2305556 (1100)\ttotal: 14.8s\tremaining: 1m 59s\n1200:\tlearn: 0.1475689\ttest: 0.2267119\tbest: 0.2267119 (1200)\ttotal: 16.2s\tremaining: 1m 58s\n1300:\tlearn: 0.1409792\ttest: 0.2232249\tbest: 0.2232249 (1300)\ttotal: 17.6s\tremaining: 1m 57s\n1400:\tlearn: 0.1353991\ttest: 0.2208733\tbest: 0.2208733 (1400)\ttotal: 18.8s\tremaining: 1m 55s\n1500:\tlearn: 0.1300277\ttest: 0.2187794\tbest: 0.2187794 (1500)\ttotal: 20.1s\tremaining: 1m 53s\n1600:\tlearn: 0.1249151\ttest: 0.2166152\tbest: 0.2166129 (1596)\ttotal: 21.3s\tremaining: 1m 51s\n1700:\tlearn: 0.1201178\ttest: 0.2145830\tbest: 0.2145177 (1697)\ttotal: 22.6s\tremaining: 1m 50s\n1800:\tlearn: 0.1152810\ttest: 0.2123411\tbest: 0.2123262 (1795)\ttotal: 24.3s\tremaining: 1m 50s\n1900:\tlearn: 0.1106353\ttest: 0.2104746\tbest: 0.2104440 (1889)\ttotal: 25.6s\tremaining: 1m 48s\n2000:\tlearn: 0.1066920\ttest: 0.2088419\tbest: 0.2088419 (2000)\ttotal: 27.7s\tremaining: 1m 50s\n2100:\tlearn: 0.1031653\ttest: 0.2077554\tbest: 0.2076768 (2094)\ttotal: 28.9s\tremaining: 1m 48s\n2200:\tlearn: 0.0993516\ttest: 0.2061252\tbest: 0.2061252 (2200)\ttotal: 30.2s\tremaining: 1m 47s\n2300:\tlearn: 0.0961806\ttest: 0.2050641\tbest: 0.2050157 (2297)\ttotal: 31.5s\tremaining: 1m 45s\n2400:\tlearn: 0.0929246\ttest: 0.2040294\tbest: 0.2039816 (2398)\ttotal: 32.9s\tremaining: 1m 44s\n2500:\tlearn: 0.0896945\ttest: 0.2033254\tbest: 0.2032034 (2497)\ttotal: 34.1s\tremaining: 1m 42s\n2600:\tlearn: 0.0867928\ttest: 0.2030826\tbest: 0.2029697 (2593)\ttotal: 35.8s\tremaining: 1m 41s\n2700:\tlearn: 0.0841228\ttest: 0.2027779\tbest: 0.2027462 (2691)\ttotal: 37.1s\tremaining: 1m 40s\n2800:\tlearn: 0.0817488\ttest: 0.2027290\tbest: 0.2027290 (2800)\ttotal: 38.3s\tremaining: 1m 38s\n2900:\tlearn: 0.0792800\ttest: 0.2023225\tbest: 0.2023090 (2871)\ttotal: 39.6s\tremaining: 1m 36s\n3000:\tlearn: 0.0769009\ttest: 0.2017866\tbest: 0.2017570 (2977)\ttotal: 40.9s\tremaining: 1m 35s\n3100:\tlearn: 0.0747296\ttest: 0.2018998\tbest: 0.2016340 (3043)\ttotal: 42.2s\tremaining: 1m 33s\n3200:\tlearn: 0.0726629\ttest: 0.2019876\tbest: 0.2016340 (3043)\ttotal: 43.5s\tremaining: 1m 32s\n3300:\tlearn: 0.0707543\ttest: 0.2015565\tbest: 0.2015057 (3297)\ttotal: 44.7s\tremaining: 1m 30s\n3400:\tlearn: 0.0687648\ttest: 0.2012481\tbest: 0.2011954 (3399)\ttotal: 46.4s\tremaining: 1m 29s\n3500:\tlearn: 0.0669338\ttest: 0.2007711\tbest: 0.2006449 (3496)\ttotal: 47.7s\tremaining: 1m 28s\n3600:\tlearn: 0.0650520\ttest: 0.2010969\tbest: 0.2006449 (3496)\ttotal: 49s\tremaining: 1m 27s\nStopped by overfitting detector  (200 iterations wait)\n\nbestTest = 0.2006449183\nbestIteration = 3496\n\nShrink model to first 3497 iterations.\n0:\tlearn: 0.6529771\ttest: 0.6531476\tbest: 0.6531476 (0)\ttotal: 10.1ms\tremaining: 1m 41s\n100:\tlearn: 0.3066466\ttest: 0.3103849\tbest: 0.3103849 (100)\ttotal: 1.2s\tremaining: 1m 58s\n200:\tlearn: 0.2819029\ttest: 0.2941178\tbest: 0.2941178 (200)\ttotal: 2.48s\tremaining: 2m 1s\n300:\tlearn: 0.2595047\ttest: 0.2799325\tbest: 0.2799325 (300)\ttotal: 3.74s\tremaining: 2m\n400:\tlearn: 0.2394108\ttest: 0.2663593\tbest: 0.2663593 (400)\ttotal: 5.03s\tremaining: 2m\n500:\tlearn: 0.2224662\ttest: 0.2549051\tbest: 0.2549051 (500)\ttotal: 6.66s\tremaining: 2m 6s\n600:\tlearn: 0.2087679\ttest: 0.2467682\tbest: 0.2467682 (600)\ttotal: 8.69s\tremaining: 2m 15s\n700:\tlearn: 0.1963520\ttest: 0.2390985\tbest: 0.2390985 (700)\ttotal: 9.97s\tremaining: 2m 12s\n800:\tlearn: 0.1854715\ttest: 0.2330399\tbest: 0.2330399 (800)\ttotal: 11.2s\tremaining: 2m 9s\n900:\tlearn: 0.1762982\ttest: 0.2287729\tbest: 0.2287729 (900)\ttotal: 12.5s\tremaining: 2m 6s\n1000:\tlearn: 0.1669827\ttest: 0.2236705\tbest: 0.2236705 (1000)\ttotal: 13.8s\tremaining: 2m 3s\n1100:\tlearn: 0.1598136\ttest: 0.2201788\tbest: 0.2201788 (1100)\ttotal: 15.1s\tremaining: 2m 1s\n1200:\tlearn: 0.1527297\ttest: 0.2166128\tbest: 0.2166128 (1200)\ttotal: 16.3s\tremaining: 1m 59s\n1300:\tlearn: 0.1455989\ttest: 0.2132366\tbest: 0.2132366 (1300)\ttotal: 18s\tremaining: 2m\n1400:\tlearn: 0.1395442\ttest: 0.2108550\tbest: 0.2108550 (1400)\ttotal: 19.2s\tremaining: 1m 58s\n1500:\tlearn: 0.1331354\ttest: 0.2075834\tbest: 0.2075834 (1500)\ttotal: 20.5s\tremaining: 1m 56s\n1600:\tlearn: 0.1278320\ttest: 0.2057703\tbest: 0.2057703 (1600)\ttotal: 21.8s\tremaining: 1m 54s\n1700:\tlearn: 0.1227914\ttest: 0.2037036\tbest: 0.2037036 (1700)\ttotal: 23.1s\tremaining: 1m 52s\n1800:\tlearn: 0.1174300\ttest: 0.2012308\tbest: 0.2012308 (1800)\ttotal: 24.4s\tremaining: 1m 50s\n1900:\tlearn: 0.1126001\ttest: 0.1990782\tbest: 0.1990782 (1900)\ttotal: 25.6s\tremaining: 1m 49s\n2000:\tlearn: 0.1085380\ttest: 0.1974704\tbest: 0.1974342 (1997)\ttotal: 26.9s\tremaining: 1m 47s\n2100:\tlearn: 0.1048409\ttest: 0.1955138\tbest: 0.1955138 (2100)\ttotal: 28.5s\tremaining: 1m 47s\n2200:\tlearn: 0.1017696\ttest: 0.1949208\tbest: 0.1948341 (2193)\ttotal: 29.7s\tremaining: 1m 45s\n2300:\tlearn: 0.0985185\ttest: 0.1940197\tbest: 0.1940197 (2300)\ttotal: 31s\tremaining: 1m 43s\n2400:\tlearn: 0.0952509\ttest: 0.1929376\tbest: 0.1929369 (2399)\ttotal: 32.2s\tremaining: 1m 42s\n2500:\tlearn: 0.0923946\ttest: 0.1924221\tbest: 0.1923774 (2483)\ttotal: 33.5s\tremaining: 1m 40s\n2600:\tlearn: 0.0894169\ttest: 0.1911547\tbest: 0.1911547 (2600)\ttotal: 34.8s\tremaining: 1m 39s\n2700:\tlearn: 0.0867401\ttest: 0.1901891\tbest: 0.1901891 (2700)\ttotal: 36.1s\tremaining: 1m 37s\n2800:\tlearn: 0.0841875\ttest: 0.1894383\tbest: 0.1893963 (2796)\ttotal: 37.3s\tremaining: 1m 35s\n2900:\tlearn: 0.0815082\ttest: 0.1888667\tbest: 0.1888225 (2897)\ttotal: 38.9s\tremaining: 1m 35s\n3000:\tlearn: 0.0789936\ttest: 0.1880422\tbest: 0.1880422 (3000)\ttotal: 41s\tremaining: 1m 35s\n3100:\tlearn: 0.0767931\ttest: 0.1873084\tbest: 0.1872066 (3092)\ttotal: 42.4s\tremaining: 1m 34s\n3200:\tlearn: 0.0746702\ttest: 0.1869219\tbest: 0.1868386 (3195)\ttotal: 43.6s\tremaining: 1m 32s\n3300:\tlearn: 0.0725580\ttest: 0.1872284\tbest: 0.1868386 (3195)\ttotal: 44.9s\tremaining: 1m 31s\nStopped by overfitting detector  (200 iterations wait)\n\nbestTest = 0.1868386142\nbestIteration = 3195\n\nShrink model to first 3196 iterations.\n0:\tlearn: 0.6530138\ttest: 0.6529250\tbest: 0.6529250 (0)\ttotal: 10.1ms\tremaining: 1m 40s\n100:\tlearn: 0.3063333\ttest: 0.3111744\tbest: 0.3111744 (100)\ttotal: 1.22s\tremaining: 1m 59s\n200:\tlearn: 0.2812437\ttest: 0.2934184\tbest: 0.2934184 (200)\ttotal: 2.5s\tremaining: 2m 1s\n300:\tlearn: 0.2586187\ttest: 0.2787364\tbest: 0.2787364 (300)\ttotal: 4.13s\tremaining: 2m 13s\n400:\tlearn: 0.2387426\ttest: 0.2666889\tbest: 0.2666889 (400)\ttotal: 5.41s\tremaining: 2m 9s\n500:\tlearn: 0.2225740\ttest: 0.2571566\tbest: 0.2571406 (499)\ttotal: 6.72s\tremaining: 2m 7s\n600:\tlearn: 0.2084481\ttest: 0.2487199\tbest: 0.2486876 (598)\ttotal: 8.03s\tremaining: 2m 5s\n700:\tlearn: 0.1958938\ttest: 0.2414795\tbest: 0.2414795 (700)\ttotal: 9.3s\tremaining: 2m 3s\n800:\tlearn: 0.1845427\ttest: 0.2358411\tbest: 0.2358411 (800)\ttotal: 10.6s\tremaining: 2m 1s\n900:\tlearn: 0.1745861\ttest: 0.2299835\tbest: 0.2299745 (899)\ttotal: 11.9s\tremaining: 2m\n1000:\tlearn: 0.1653315\ttest: 0.2249459\tbest: 0.2249459 (1000)\ttotal: 13.2s\tremaining: 1m 58s\n1100:\tlearn: 0.1569292\ttest: 0.2208838\tbest: 0.2208838 (1100)\ttotal: 14.9s\tremaining: 2m\n1200:\tlearn: 0.1497863\ttest: 0.2172977\tbest: 0.2172977 (1200)\ttotal: 16.2s\tremaining: 1m 58s\n1300:\tlearn: 0.1432697\ttest: 0.2144350\tbest: 0.2144350 (1300)\ttotal: 17.5s\tremaining: 1m 56s\n1400:\tlearn: 0.1371298\ttest: 0.2118405\tbest: 0.2118272 (1398)\ttotal: 18.8s\tremaining: 1m 55s\n1500:\tlearn: 0.1314323\ttest: 0.2089681\tbest: 0.2089651 (1499)\ttotal: 20.3s\tremaining: 1m 54s\n1600:\tlearn: 0.1262158\ttest: 0.2066310\tbest: 0.2065673 (1597)\ttotal: 21.5s\tremaining: 1m 52s\n1700:\tlearn: 0.1213113\ttest: 0.2043068\tbest: 0.2043068 (1700)\ttotal: 22.8s\tremaining: 1m 51s\n1800:\tlearn: 0.1168495\ttest: 0.2027837\tbest: 0.2027639 (1798)\ttotal: 24.1s\tremaining: 1m 49s\n1900:\tlearn: 0.1122839\ttest: 0.2008300\tbest: 0.2008300 (1900)\ttotal: 26.4s\tremaining: 1m 52s\n2000:\tlearn: 0.1081466\ttest: 0.1992010\tbest: 0.1992010 (2000)\ttotal: 27.7s\tremaining: 1m 50s\n2100:\tlearn: 0.1045870\ttest: 0.1986406\tbest: 0.1985432 (2098)\ttotal: 29s\tremaining: 1m 49s\n2200:\tlearn: 0.1008893\ttest: 0.1966017\tbest: 0.1965991 (2199)\ttotal: 30.3s\tremaining: 1m 47s\n2300:\tlearn: 0.0973493\ttest: 0.1955017\tbest: 0.1955017 (2300)\ttotal: 31.5s\tremaining: 1m 45s\n2400:\tlearn: 0.0940823\ttest: 0.1945659\tbest: 0.1945273 (2392)\ttotal: 32.8s\tremaining: 1m 43s\n2500:\tlearn: 0.0907292\ttest: 0.1931235\tbest: 0.1930777 (2497)\ttotal: 34.1s\tremaining: 1m 42s\n2600:\tlearn: 0.0877776\ttest: 0.1920731\tbest: 0.1920731 (2600)\ttotal: 35.5s\tremaining: 1m 40s\n2700:\tlearn: 0.0850053\ttest: 0.1915715\tbest: 0.1915715 (2700)\ttotal: 37.1s\tremaining: 1m 40s\n2800:\tlearn: 0.0824582\ttest: 0.1909227\tbest: 0.1908814 (2797)\ttotal: 38.4s\tremaining: 1m 38s\n2900:\tlearn: 0.0801784\ttest: 0.1903792\tbest: 0.1903792 (2900)\ttotal: 39.7s\tremaining: 1m 37s\n3000:\tlearn: 0.0779157\ttest: 0.1900785\tbest: 0.1900661 (2992)\ttotal: 40.9s\tremaining: 1m 35s\n3100:\tlearn: 0.0752960\ttest: 0.1894440\tbest: 0.1894279 (3086)\ttotal: 42.2s\tremaining: 1m 33s\n3200:\tlearn: 0.0732392\ttest: 0.1894203\tbest: 0.1892993 (3182)\ttotal: 43.6s\tremaining: 1m 32s\n3300:\tlearn: 0.0711426\ttest: 0.1892065\tbest: 0.1891878 (3287)\ttotal: 44.9s\tremaining: 1m 31s\n3400:\tlearn: 0.0690810\ttest: 0.1887857\tbest: 0.1887292 (3395)\ttotal: 46.2s\tremaining: 1m 29s\n3500:\tlearn: 0.0672508\ttest: 0.1887991\tbest: 0.1887292 (3395)\ttotal: 47.8s\tremaining: 1m 28s\n3600:\tlearn: 0.0655561\ttest: 0.1882745\tbest: 0.1882136 (3574)\ttotal: 49.1s\tremaining: 1m 27s\n3700:\tlearn: 0.0636423\ttest: 0.1883110\tbest: 0.1881828 (3648)\ttotal: 50.4s\tremaining: 1m 25s\n3800:\tlearn: 0.0618409\ttest: 0.1885291\tbest: 0.1881828 (3648)\ttotal: 51.7s\tremaining: 1m 24s\nStopped by overfitting detector  (200 iterations wait)\n\nbestTest = 0.1881827716\nbestIteration = 3648\n\nShrink model to first 3649 iterations.\n0:\tlearn: 0.6529675\ttest: 0.6530891\tbest: 0.6530891 (0)\ttotal: 9.26ms\tremaining: 1m 32s\n100:\tlearn: 0.3056026\ttest: 0.3145308\tbest: 0.3145308 (100)\ttotal: 1.2s\tremaining: 1m 57s\n200:\tlearn: 0.2795849\ttest: 0.2978762\tbest: 0.2978762 (200)\ttotal: 2.54s\tremaining: 2m 3s\n300:\tlearn: 0.2564033\ttest: 0.2846172\tbest: 0.2845401 (299)\ttotal: 3.88s\tremaining: 2m 4s\n400:\tlearn: 0.2379723\ttest: 0.2741358\tbest: 0.2741358 (400)\ttotal: 6.16s\tremaining: 2m 27s\n500:\tlearn: 0.2209698\ttest: 0.2643224\tbest: 0.2643224 (500)\ttotal: 7.45s\tremaining: 2m 21s\n600:\tlearn: 0.2061926\ttest: 0.2551729\tbest: 0.2551729 (600)\ttotal: 8.74s\tremaining: 2m 16s\n700:\tlearn: 0.1941318\ttest: 0.2483259\tbest: 0.2483259 (700)\ttotal: 10s\tremaining: 2m 13s\n800:\tlearn: 0.1823077\ttest: 0.2410800\tbest: 0.2410800 (800)\ttotal: 11.4s\tremaining: 2m 10s\n900:\tlearn: 0.1724063\ttest: 0.2355951\tbest: 0.2355951 (900)\ttotal: 12.7s\tremaining: 2m 7s\n1000:\tlearn: 0.1633600\ttest: 0.2305187\tbest: 0.2305187 (1000)\ttotal: 13.9s\tremaining: 2m 5s\n1100:\tlearn: 0.1557813\ttest: 0.2269099\tbest: 0.2269099 (1100)\ttotal: 15.2s\tremaining: 2m 2s\n1200:\tlearn: 0.1490457\ttest: 0.2240154\tbest: 0.2240154 (1200)\ttotal: 16.8s\tremaining: 2m 3s\n1300:\tlearn: 0.1421451\ttest: 0.2207900\tbest: 0.2207900 (1300)\ttotal: 18.1s\tremaining: 2m 1s\n1400:\tlearn: 0.1362312\ttest: 0.2184640\tbest: 0.2184344 (1398)\ttotal: 19.4s\tremaining: 1m 59s\n1500:\tlearn: 0.1303308\ttest: 0.2156851\tbest: 0.2156851 (1500)\ttotal: 20.8s\tremaining: 1m 57s\n1600:\tlearn: 0.1250416\ttest: 0.2132530\tbest: 0.2132530 (1600)\ttotal: 22.1s\tremaining: 1m 55s\n1700:\tlearn: 0.1202059\ttest: 0.2111916\tbest: 0.2111916 (1700)\ttotal: 23.5s\tremaining: 1m 54s\n1800:\tlearn: 0.1152080\ttest: 0.2093158\tbest: 0.2092434 (1796)\ttotal: 24.8s\tremaining: 1m 53s\n1900:\tlearn: 0.1109259\ttest: 0.2071857\tbest: 0.2071857 (1900)\ttotal: 26.1s\tremaining: 1m 51s\n2000:\tlearn: 0.1069181\ttest: 0.2055864\tbest: 0.2055864 (2000)\ttotal: 27.8s\tremaining: 1m 51s\n2100:\tlearn: 0.1030587\ttest: 0.2041753\tbest: 0.2041753 (2100)\ttotal: 29.1s\tremaining: 1m 49s\n2200:\tlearn: 0.0995409\ttest: 0.2029405\tbest: 0.2029323 (2198)\ttotal: 30.3s\tremaining: 1m 47s\n2300:\tlearn: 0.0961010\ttest: 0.2017264\tbest: 0.2016840 (2294)\ttotal: 31.6s\tremaining: 1m 45s\n2400:\tlearn: 0.0930232\ttest: 0.2010493\tbest: 0.2010469 (2399)\ttotal: 32.9s\tremaining: 1m 44s\n2500:\tlearn: 0.0902226\ttest: 0.2003712\tbest: 0.2003712 (2500)\ttotal: 34.1s\tremaining: 1m 42s\n2600:\tlearn: 0.0872249\ttest: 0.1996967\tbest: 0.1995615 (2592)\ttotal: 35.5s\tremaining: 1m 41s\n2700:\tlearn: 0.0845071\ttest: 0.1987857\tbest: 0.1987857 (2700)\ttotal: 37.6s\tremaining: 1m 41s\n2800:\tlearn: 0.0820756\ttest: 0.1985358\tbest: 0.1984250 (2752)\ttotal: 39.3s\tremaining: 1m 40s\n2900:\tlearn: 0.0794695\ttest: 0.1979787\tbest: 0.1979179 (2878)\ttotal: 40.6s\tremaining: 1m 39s\n3000:\tlearn: 0.0772069\ttest: 0.1974433\tbest: 0.1974331 (2999)\ttotal: 41.9s\tremaining: 1m 37s\n3100:\tlearn: 0.0749138\ttest: 0.1970524\tbest: 0.1970125 (3090)\ttotal: 43.2s\tremaining: 1m 36s\n3200:\tlearn: 0.0728324\ttest: 0.1967965\tbest: 0.1967581 (3135)\ttotal: 44.5s\tremaining: 1m 34s\n3300:\tlearn: 0.0708516\ttest: 0.1965589\tbest: 0.1965095 (3293)\ttotal: 45.9s\tremaining: 1m 33s\n3400:\tlearn: 0.0689119\ttest: 0.1964446\tbest: 0.1964437 (3385)\ttotal: 47.2s\tremaining: 1m 31s\n3500:\tlearn: 0.0670625\ttest: 0.1962229\tbest: 0.1961698 (3497)\ttotal: 48.5s\tremaining: 1m 29s\n3600:\tlearn: 0.0652009\ttest: 0.1959376\tbest: 0.1958873 (3583)\ttotal: 50.1s\tremaining: 1m 28s\n3700:\tlearn: 0.0633620\ttest: 0.1954382\tbest: 0.1954382 (3700)\ttotal: 51.5s\tremaining: 1m 27s\n3800:\tlearn: 0.0616030\ttest: 0.1953706\tbest: 0.1950938 (3728)\ttotal: 52.8s\tremaining: 1m 26s\n3900:\tlearn: 0.0601072\ttest: 0.1952136\tbest: 0.1950938 (3728)\ttotal: 54s\tremaining: 1m 24s\nStopped by overfitting detector  (200 iterations wait)\n\nbestTest = 0.1950938478\nbestIteration = 3728\n\nShrink model to first 3729 iterations.\n0:\tlearn: 0.6530172\ttest: 0.6529383\tbest: 0.6529383 (0)\ttotal: 8.85ms\tremaining: 1m 28s\n100:\tlearn: 0.3078534\ttest: 0.3092107\tbest: 0.3092107 (100)\ttotal: 1.2s\tremaining: 1m 57s\n200:\tlearn: 0.2827479\ttest: 0.2923617\tbest: 0.2923617 (200)\ttotal: 2.47s\tremaining: 2m\n300:\tlearn: 0.2584526\ttest: 0.2766456\tbest: 0.2766456 (300)\ttotal: 3.73s\tremaining: 2m\n400:\tlearn: 0.2381495\ttest: 0.2632253\tbest: 0.2632253 (400)\ttotal: 5.03s\tremaining: 2m\n500:\tlearn: 0.2215774\ttest: 0.2526032\tbest: 0.2526032 (500)\ttotal: 6.66s\tremaining: 2m 6s\n600:\tlearn: 0.2073683\ttest: 0.2433406\tbest: 0.2433406 (600)\ttotal: 8s\tremaining: 2m 5s\n700:\tlearn: 0.1960065\ttest: 0.2360033\tbest: 0.2360033 (700)\ttotal: 9.28s\tremaining: 2m 3s\n800:\tlearn: 0.1856976\ttest: 0.2293567\tbest: 0.2293567 (800)\ttotal: 10.5s\tremaining: 2m 1s\n900:\tlearn: 0.1758499\ttest: 0.2232047\tbest: 0.2232047 (900)\ttotal: 11.8s\tremaining: 1m 59s\n1000:\tlearn: 0.1667584\ttest: 0.2180411\tbest: 0.2180411 (1000)\ttotal: 13.9s\tremaining: 2m 4s\n1100:\tlearn: 0.1586458\ttest: 0.2127988\tbest: 0.2127988 (1100)\ttotal: 15.1s\tremaining: 2m 2s\n1200:\tlearn: 0.1516804\ttest: 0.2085266\tbest: 0.2085266 (1200)\ttotal: 16.6s\tremaining: 2m 1s\n1300:\tlearn: 0.1448251\ttest: 0.2048343\tbest: 0.2048343 (1300)\ttotal: 18s\tremaining: 2m\n1400:\tlearn: 0.1388524\ttest: 0.2017551\tbest: 0.2017551 (1400)\ttotal: 19.3s\tremaining: 1m 58s\n1500:\tlearn: 0.1332963\ttest: 0.1992925\tbest: 0.1992662 (1499)\ttotal: 20.6s\tremaining: 1m 56s\n1600:\tlearn: 0.1279482\ttest: 0.1974102\tbest: 0.1974076 (1597)\ttotal: 21.8s\tremaining: 1m 54s\n1700:\tlearn: 0.1229278\ttest: 0.1952647\tbest: 0.1952647 (1700)\ttotal: 23.1s\tremaining: 1m 52s\n1800:\tlearn: 0.1183927\ttest: 0.1934916\tbest: 0.1934598 (1798)\ttotal: 24.4s\tremaining: 1m 51s\n1900:\tlearn: 0.1135816\ttest: 0.1910529\tbest: 0.1910529 (1900)\ttotal: 25.7s\tremaining: 1m 49s\n2000:\tlearn: 0.1099092\ttest: 0.1895360\tbest: 0.1895360 (2000)\ttotal: 27s\tremaining: 1m 48s\n2100:\tlearn: 0.1061877\ttest: 0.1881907\tbest: 0.1881907 (2100)\ttotal: 28.7s\tremaining: 1m 47s\n2200:\tlearn: 0.1028099\ttest: 0.1871547\tbest: 0.1871547 (2200)\ttotal: 30s\tremaining: 1m 46s\n2300:\tlearn: 0.0991828\ttest: 0.1853261\tbest: 0.1853261 (2300)\ttotal: 31.3s\tremaining: 1m 44s\n2400:\tlearn: 0.0959640\ttest: 0.1842507\tbest: 0.1842507 (2400)\ttotal: 32.5s\tremaining: 1m 42s\n2500:\tlearn: 0.0928734\ttest: 0.1825498\tbest: 0.1824792 (2498)\ttotal: 33.8s\tremaining: 1m 41s\n2600:\tlearn: 0.0899759\ttest: 0.1816834\tbest: 0.1815704 (2594)\ttotal: 35.1s\tremaining: 1m 39s\n2700:\tlearn: 0.0869967\ttest: 0.1805164\tbest: 0.1805164 (2700)\ttotal: 36.3s\tremaining: 1m 38s\n2800:\tlearn: 0.0842729\ttest: 0.1800696\tbest: 0.1800525 (2798)\ttotal: 37.6s\tremaining: 1m 36s\n2900:\tlearn: 0.0816870\ttest: 0.1794816\tbest: 0.1794507 (2897)\ttotal: 39.3s\tremaining: 1m 36s\n3000:\tlearn: 0.0793511\ttest: 0.1788703\tbest: 0.1788482 (2999)\ttotal: 40.5s\tremaining: 1m 34s\n3100:\tlearn: 0.0769321\ttest: 0.1779809\tbest: 0.1779700 (3097)\ttotal: 41.8s\tremaining: 1m 33s\n3200:\tlearn: 0.0746736\ttest: 0.1774616\tbest: 0.1774585 (3147)\ttotal: 43.1s\tremaining: 1m 31s\n3300:\tlearn: 0.0726532\ttest: 0.1770421\tbest: 0.1769494 (3291)\ttotal: 44.5s\tremaining: 1m 30s\n3400:\tlearn: 0.0703942\ttest: 0.1761804\tbest: 0.1761670 (3384)\ttotal: 46.5s\tremaining: 1m 30s\n3500:\tlearn: 0.0685043\ttest: 0.1756641\tbest: 0.1755785 (3477)\ttotal: 47.9s\tremaining: 1m 28s\n3600:\tlearn: 0.0667112\ttest: 0.1756697\tbest: 0.1754911 (3516)\ttotal: 49.3s\tremaining: 1m 27s\n3700:\tlearn: 0.0648004\ttest: 0.1751788\tbest: 0.1751738 (3697)\ttotal: 50.8s\tremaining: 1m 26s\n3800:\tlearn: 0.0630637\ttest: 0.1750085\tbest: 0.1748736 (3749)\ttotal: 52.1s\tremaining: 1m 24s\n3900:\tlearn: 0.0614937\ttest: 0.1748124\tbest: 0.1747129 (3890)\ttotal: 53.3s\tremaining: 1m 23s\n4000:\tlearn: 0.0599739\ttest: 0.1745477\tbest: 0.1745089 (3986)\ttotal: 54.6s\tremaining: 1m 21s\n4100:\tlearn: 0.0584493\ttest: 0.1745555\tbest: 0.1744165 (4029)\ttotal: 55.9s\tremaining: 1m 20s\n4200:\tlearn: 0.0568865\ttest: 0.1740134\tbest: 0.1739765 (4198)\ttotal: 57.3s\tremaining: 1m 19s\n4300:\tlearn: 0.0553829\ttest: 0.1739139\tbest: 0.1738549 (4285)\ttotal: 58.5s\tremaining: 1m 17s\n4400:\tlearn: 0.0539955\ttest: 0.1739626\tbest: 0.1737252 (4338)\ttotal: 59.8s\tremaining: 1m 16s\n4500:\tlearn: 0.0527650\ttest: 0.1740388\tbest: 0.1737252 (4338)\ttotal: 1m 1s\tremaining: 1m 15s\nStopped by overfitting detector  (200 iterations wait)\n\nbestTest = 0.1737251979\nbestIteration = 4338\n\nShrink model to first 4339 iterations.\n0:\tlearn: 0.6529714\ttest: 0.6530666\tbest: 0.6530666 (0)\ttotal: 9.06ms\tremaining: 1m 30s\n100:\tlearn: 0.3065980\ttest: 0.3107896\tbest: 0.3107896 (100)\ttotal: 1.22s\tremaining: 1m 59s\n200:\tlearn: 0.2810874\ttest: 0.2936102\tbest: 0.2936102 (200)\ttotal: 2.5s\tremaining: 2m 2s\n300:\tlearn: 0.2566009\ttest: 0.2775930\tbest: 0.2775930 (300)\ttotal: 3.78s\tremaining: 2m 1s\n400:\tlearn: 0.2370105\ttest: 0.2658622\tbest: 0.2658622 (400)\ttotal: 5.12s\tremaining: 2m 2s\n500:\tlearn: 0.2197333\ttest: 0.2559011\tbest: 0.2559011 (500)\ttotal: 6.42s\tremaining: 2m 1s\n600:\tlearn: 0.2058633\ttest: 0.2481060\tbest: 0.2481060 (600)\ttotal: 7.72s\tremaining: 2m\n700:\tlearn: 0.1946851\ttest: 0.2420493\tbest: 0.2420493 (700)\ttotal: 9.18s\tremaining: 2m 1s\n800:\tlearn: 0.1829470\ttest: 0.2353640\tbest: 0.2353640 (800)\ttotal: 10.6s\tremaining: 2m 1s\n900:\tlearn: 0.1733211\ttest: 0.2303981\tbest: 0.2303748 (899)\ttotal: 11.8s\tremaining: 1m 59s\n1000:\tlearn: 0.1643099\ttest: 0.2265396\tbest: 0.2265396 (1000)\ttotal: 13.1s\tremaining: 1m 57s\n1100:\tlearn: 0.1562171\ttest: 0.2227839\tbest: 0.2227653 (1098)\ttotal: 15.2s\tremaining: 2m 2s\n1200:\tlearn: 0.1494961\ttest: 0.2198786\tbest: 0.2198786 (1200)\ttotal: 16.5s\tremaining: 2m\n1300:\tlearn: 0.1428411\ttest: 0.2162297\tbest: 0.2162297 (1300)\ttotal: 17.7s\tremaining: 1m 58s\n1400:\tlearn: 0.1364890\ttest: 0.2133093\tbest: 0.2133093 (1400)\ttotal: 19s\tremaining: 1m 56s\n1500:\tlearn: 0.1313434\ttest: 0.2112050\tbest: 0.2111752 (1499)\ttotal: 20.7s\tremaining: 1m 57s\n1600:\tlearn: 0.1256249\ttest: 0.2090454\tbest: 0.2088773 (1593)\ttotal: 22s\tremaining: 1m 55s\n1700:\tlearn: 0.1207792\ttest: 0.2070008\tbest: 0.2070008 (1700)\ttotal: 23.2s\tremaining: 1m 53s\n1800:\tlearn: 0.1161903\ttest: 0.2052021\tbest: 0.2052021 (1800)\ttotal: 24.5s\tremaining: 1m 51s\n1900:\tlearn: 0.1114780\ttest: 0.2029913\tbest: 0.2029803 (1898)\ttotal: 25.9s\tremaining: 1m 50s\n2000:\tlearn: 0.1075481\ttest: 0.2017380\tbest: 0.2017380 (2000)\ttotal: 27.2s\tremaining: 1m 48s\n2100:\tlearn: 0.1033291\ttest: 0.2000000\tbest: 0.1999868 (2099)\ttotal: 28.5s\tremaining: 1m 47s\n2200:\tlearn: 0.0996021\ttest: 0.1985216\tbest: 0.1984928 (2199)\ttotal: 29.8s\tremaining: 1m 45s\n2300:\tlearn: 0.0958336\ttest: 0.1971112\tbest: 0.1971112 (2300)\ttotal: 31.4s\tremaining: 1m 45s\n2400:\tlearn: 0.0929184\ttest: 0.1968622\tbest: 0.1968219 (2394)\ttotal: 32.6s\tremaining: 1m 43s\n2500:\tlearn: 0.0901272\ttest: 0.1960510\tbest: 0.1960039 (2494)\ttotal: 33.9s\tremaining: 1m 41s\n2600:\tlearn: 0.0873470\ttest: 0.1955988\tbest: 0.1955962 (2580)\ttotal: 35.2s\tremaining: 1m 40s\n2700:\tlearn: 0.0845944\ttest: 0.1949822\tbest: 0.1949822 (2700)\ttotal: 36.5s\tremaining: 1m 38s\n2800:\tlearn: 0.0820157\ttest: 0.1943976\tbest: 0.1943764 (2799)\ttotal: 37.7s\tremaining: 1m 36s\n2900:\tlearn: 0.0797035\ttest: 0.1939769\tbest: 0.1939035 (2877)\ttotal: 39s\tremaining: 1m 35s\n3000:\tlearn: 0.0771763\ttest: 0.1939911\tbest: 0.1938882 (2995)\ttotal: 40.3s\tremaining: 1m 34s\n3100:\tlearn: 0.0747597\ttest: 0.1942237\tbest: 0.1938882 (2995)\ttotal: 41.8s\tremaining: 1m 32s\nStopped by overfitting detector  (200 iterations wait)\n\nbestTest = 0.1938881536\nbestIteration = 2995\n\nShrink model to first 2996 iterations.\n0:\tlearn: 0.6529427\ttest: 0.6531090\tbest: 0.6531090 (0)\ttotal: 9.79ms\tremaining: 1m 37s\n100:\tlearn: 0.3056876\ttest: 0.3154632\tbest: 0.3154632 (100)\ttotal: 1.23s\tremaining: 2m\n200:\tlearn: 0.2811721\ttest: 0.2994804\tbest: 0.2994804 (200)\ttotal: 2.77s\tremaining: 2m 15s\n300:\tlearn: 0.2580485\ttest: 0.2866712\tbest: 0.2866712 (300)\ttotal: 4.54s\tremaining: 2m 26s\n400:\tlearn: 0.2375006\ttest: 0.2747030\tbest: 0.2747030 (400)\ttotal: 5.85s\tremaining: 2m 19s\n500:\tlearn: 0.2206832\ttest: 0.2655072\tbest: 0.2654828 (499)\ttotal: 7.13s\tremaining: 2m 15s\n600:\tlearn: 0.2063303\ttest: 0.2574276\tbest: 0.2574276 (600)\ttotal: 8.39s\tremaining: 2m 11s\n700:\tlearn: 0.1939205\ttest: 0.2509252\tbest: 0.2509252 (700)\ttotal: 10s\tremaining: 2m 13s\n800:\tlearn: 0.1826310\ttest: 0.2440767\tbest: 0.2440767 (800)\ttotal: 11.3s\tremaining: 2m 9s\n900:\tlearn: 0.1726111\ttest: 0.2391230\tbest: 0.2391230 (900)\ttotal: 12.7s\tremaining: 2m 8s\n1000:\tlearn: 0.1637128\ttest: 0.2348050\tbest: 0.2347603 (999)\ttotal: 14s\tremaining: 2m 5s\n1100:\tlearn: 0.1560978\ttest: 0.2311335\tbest: 0.2311335 (1100)\ttotal: 15.3s\tremaining: 2m 3s\n1200:\tlearn: 0.1491774\ttest: 0.2274721\tbest: 0.2274721 (1200)\ttotal: 16.5s\tremaining: 2m\n1300:\tlearn: 0.1424630\ttest: 0.2246597\tbest: 0.2246172 (1298)\ttotal: 17.8s\tremaining: 1m 58s\n1400:\tlearn: 0.1364030\ttest: 0.2224077\tbest: 0.2224077 (1400)\ttotal: 19.1s\tremaining: 1m 56s\n1500:\tlearn: 0.1302267\ttest: 0.2190355\tbest: 0.2190355 (1500)\ttotal: 20.6s\tremaining: 1m 56s\n1600:\tlearn: 0.1247171\ttest: 0.2172250\tbest: 0.2171927 (1599)\ttotal: 22s\tremaining: 1m 55s\n1700:\tlearn: 0.1198613\ttest: 0.2150651\tbest: 0.2150651 (1700)\ttotal: 23.3s\tremaining: 1m 53s\n1800:\tlearn: 0.1150204\ttest: 0.2126810\tbest: 0.2126810 (1800)\ttotal: 24.6s\tremaining: 1m 52s\n1900:\tlearn: 0.1108703\ttest: 0.2111036\tbest: 0.2111036 (1900)\ttotal: 25.9s\tremaining: 1m 50s\n2000:\tlearn: 0.1065352\ttest: 0.2104147\tbest: 0.2103831 (1965)\ttotal: 27.2s\tremaining: 1m 48s\n2100:\tlearn: 0.1027727\ttest: 0.2091256\tbest: 0.2091254 (2099)\ttotal: 28.4s\tremaining: 1m 46s\n2200:\tlearn: 0.0991395\ttest: 0.2081224\tbest: 0.2081185 (2199)\ttotal: 29.7s\tremaining: 1m 45s\n2300:\tlearn: 0.0959083\ttest: 0.2066038\tbest: 0.2066038 (2300)\ttotal: 31.1s\tremaining: 1m 43s\n2400:\tlearn: 0.0926979\ttest: 0.2059550\tbest: 0.2059516 (2370)\ttotal: 32.6s\tremaining: 1m 43s\n2500:\tlearn: 0.0895253\ttest: 0.2050052\tbest: 0.2049631 (2490)\ttotal: 33.9s\tremaining: 1m 41s\n2600:\tlearn: 0.0871119\ttest: 0.2042046\tbest: 0.2042046 (2600)\ttotal: 36s\tremaining: 1m 42s\n2700:\tlearn: 0.0842564\ttest: 0.2033555\tbest: 0.2033080 (2694)\ttotal: 37.3s\tremaining: 1m 40s\n2800:\tlearn: 0.0815283\ttest: 0.2023844\tbest: 0.2022677 (2794)\ttotal: 38.6s\tremaining: 1m 39s\n2900:\tlearn: 0.0791706\ttest: 0.2022055\tbest: 0.2021560 (2869)\ttotal: 39.8s\tremaining: 1m 37s\n3000:\tlearn: 0.0768734\ttest: 0.2019859\tbest: 0.2019764 (2999)\ttotal: 41.1s\tremaining: 1m 35s\n3100:\tlearn: 0.0747188\ttest: 0.2017843\tbest: 0.2015215 (3081)\ttotal: 42.8s\tremaining: 1m 35s\n3200:\tlearn: 0.0726820\ttest: 0.2014005\tbest: 0.2013327 (3178)\ttotal: 44.1s\tremaining: 1m 33s\n3300:\tlearn: 0.0704941\ttest: 0.2009622\tbest: 0.2009404 (3299)\ttotal: 45.4s\tremaining: 1m 32s\n3400:\tlearn: 0.0683843\ttest: 0.2007925\tbest: 0.2007098 (3390)\ttotal: 46.7s\tremaining: 1m 30s\n3500:\tlearn: 0.0662625\ttest: 0.2006430\tbest: 0.2005336 (3486)\ttotal: 48s\tremaining: 1m 29s\n3600:\tlearn: 0.0644212\ttest: 0.2004284\tbest: 0.2004284 (3600)\ttotal: 49.3s\tremaining: 1m 27s\n3700:\tlearn: 0.0626718\ttest: 0.1999533\tbest: 0.1999123 (3694)\ttotal: 50.5s\tremaining: 1m 26s\n3800:\tlearn: 0.0610989\ttest: 0.2001072\tbest: 0.1999123 (3694)\ttotal: 51.8s\tremaining: 1m 24s\nStopped by overfitting detector  (200 iterations wait)\n\nbestTest = 0.1999123333\nbestIteration = 3694\n\nShrink model to first 3695 iterations.\n0:\tlearn: 0.6529760\ttest: 0.6531003\tbest: 0.6531003 (0)\ttotal: 9.54ms\tremaining: 1m 35s\n100:\tlearn: 0.3057813\ttest: 0.3159745\tbest: 0.3159745 (100)\ttotal: 1.26s\tremaining: 2m 3s\n200:\tlearn: 0.2806469\ttest: 0.2992746\tbest: 0.2992746 (200)\ttotal: 2.54s\tremaining: 2m 3s\n300:\tlearn: 0.2577586\ttest: 0.2849151\tbest: 0.2849151 (300)\ttotal: 3.78s\tremaining: 2m 1s\n400:\tlearn: 0.2380775\ttest: 0.2734211\tbest: 0.2734211 (400)\ttotal: 5.05s\tremaining: 2m\n500:\tlearn: 0.2215284\ttest: 0.2618118\tbest: 0.2618118 (500)\ttotal: 6.34s\tremaining: 2m\n600:\tlearn: 0.2083549\ttest: 0.2538941\tbest: 0.2538941 (600)\ttotal: 7.63s\tremaining: 1m 59s\n700:\tlearn: 0.1968096\ttest: 0.2478760\tbest: 0.2478755 (696)\ttotal: 8.9s\tremaining: 1m 58s\n800:\tlearn: 0.1862338\ttest: 0.2419927\tbest: 0.2419927 (800)\ttotal: 10.2s\tremaining: 1m 56s\n900:\tlearn: 0.1755275\ttest: 0.2351317\tbest: 0.2351317 (900)\ttotal: 11.8s\tremaining: 1m 59s\n1000:\tlearn: 0.1671652\ttest: 0.2303375\tbest: 0.2303375 (1000)\ttotal: 13.9s\tremaining: 2m 4s\n1100:\tlearn: 0.1593577\ttest: 0.2262613\tbest: 0.2262613 (1100)\ttotal: 15.2s\tremaining: 2m 2s\n1200:\tlearn: 0.1520992\ttest: 0.2221541\tbest: 0.2221541 (1200)\ttotal: 16.4s\tremaining: 2m\n1300:\tlearn: 0.1454537\ttest: 0.2184264\tbest: 0.2184264 (1300)\ttotal: 17.7s\tremaining: 1m 58s\n1400:\tlearn: 0.1389123\ttest: 0.2153443\tbest: 0.2153305 (1399)\ttotal: 19s\tremaining: 1m 56s\n1500:\tlearn: 0.1331210\ttest: 0.2123037\tbest: 0.2123037 (1500)\ttotal: 20.4s\tremaining: 1m 55s\n1600:\tlearn: 0.1279704\ttest: 0.2103816\tbest: 0.2103816 (1600)\ttotal: 22s\tremaining: 1m 55s\n1700:\tlearn: 0.1227723\ttest: 0.2081566\tbest: 0.2081566 (1700)\ttotal: 23.3s\tremaining: 1m 53s\n1800:\tlearn: 0.1179855\ttest: 0.2061079\tbest: 0.2061079 (1800)\ttotal: 24.5s\tremaining: 1m 51s\n1900:\tlearn: 0.1136679\ttest: 0.2042494\tbest: 0.2042494 (1900)\ttotal: 25.8s\tremaining: 1m 49s\n2000:\tlearn: 0.1092955\ttest: 0.2024226\tbest: 0.2023811 (1996)\ttotal: 27.1s\tremaining: 1m 48s\n2100:\tlearn: 0.1054909\ttest: 0.2014540\tbest: 0.2014540 (2100)\ttotal: 28.4s\tremaining: 1m 46s\n2200:\tlearn: 0.1017590\ttest: 0.2003848\tbest: 0.2002804 (2193)\ttotal: 29.7s\tremaining: 1m 45s\n2300:\tlearn: 0.0982596\ttest: 0.1987984\tbest: 0.1987882 (2299)\ttotal: 30.9s\tremaining: 1m 43s\n2400:\tlearn: 0.0950505\ttest: 0.1978392\tbest: 0.1977641 (2397)\ttotal: 32.4s\tremaining: 1m 42s\n2500:\tlearn: 0.0914301\ttest: 0.1965619\tbest: 0.1965553 (2497)\ttotal: 33.9s\tremaining: 1m 41s\n2600:\tlearn: 0.0887046\ttest: 0.1966048\tbest: 0.1964724 (2527)\ttotal: 35.1s\tremaining: 1m 39s\n2700:\tlearn: 0.0862054\ttest: 0.1959089\tbest: 0.1958728 (2694)\ttotal: 36.4s\tremaining: 1m 38s\n2800:\tlearn: 0.0834991\ttest: 0.1959108\tbest: 0.1957538 (2774)\ttotal: 37.7s\tremaining: 1m 36s\n2900:\tlearn: 0.0809781\ttest: 0.1954332\tbest: 0.1954332 (2900)\ttotal: 39s\tremaining: 1m 35s\n3000:\tlearn: 0.0785778\ttest: 0.1950131\tbest: 0.1950131 (3000)\ttotal: 40.2s\tremaining: 1m 33s\n3100:\tlearn: 0.0761539\ttest: 0.1947687\tbest: 0.1947570 (3080)\ttotal: 41.5s\tremaining: 1m 32s\n3200:\tlearn: 0.0739334\ttest: 0.1946099\tbest: 0.1945640 (3123)\ttotal: 42.8s\tremaining: 1m 30s\n3300:\tlearn: 0.0717763\ttest: 0.1946291\tbest: 0.1945640 (3123)\ttotal: 45.1s\tremaining: 1m 31s\n3400:\tlearn: 0.0698731\ttest: 0.1940154\tbest: 0.1939884 (3398)\ttotal: 46.4s\tremaining: 1m 30s\n3500:\tlearn: 0.0679575\ttest: 0.1939027\tbest: 0.1938521 (3497)\ttotal: 47.7s\tremaining: 1m 28s\n3600:\tlearn: 0.0660814\ttest: 0.1937932\tbest: 0.1937050 (3588)\ttotal: 49s\tremaining: 1m 27s\n3700:\tlearn: 0.0641863\ttest: 0.1936228\tbest: 0.1934083 (3649)\ttotal: 50.4s\tremaining: 1m 25s\n3800:\tlearn: 0.0624188\ttest: 0.1935647\tbest: 0.1934083 (3649)\ttotal: 51.7s\tremaining: 1m 24s\n3900:\tlearn: 0.0608675\ttest: 0.1933854\tbest: 0.1932839 (3886)\ttotal: 53s\tremaining: 1m 22s\n4000:\tlearn: 0.0592846\ttest: 0.1935255\tbest: 0.1932839 (3886)\ttotal: 54.5s\tremaining: 1m 21s\nStopped by overfitting detector  (200 iterations wait)\n\nbestTest = 0.1932838688\nbestIteration = 3886\n\nShrink model to first 3887 iterations.\n0:\tlearn: 0.6529976\ttest: 0.6530279\tbest: 0.6530279 (0)\ttotal: 8.71ms\tremaining: 1m 27s\n100:\tlearn: 0.3057212\ttest: 0.3136051\tbest: 0.3136051 (100)\ttotal: 1.21s\tremaining: 1m 58s\n200:\tlearn: 0.2807836\ttest: 0.2965664\tbest: 0.2965664 (200)\ttotal: 2.47s\tremaining: 2m\n300:\tlearn: 0.2575807\ttest: 0.2808235\tbest: 0.2808235 (300)\ttotal: 3.73s\tremaining: 2m\n400:\tlearn: 0.2364723\ttest: 0.2671451\tbest: 0.2671451 (400)\ttotal: 5.03s\tremaining: 2m\n500:\tlearn: 0.2202616\ttest: 0.2564901\tbest: 0.2564901 (500)\ttotal: 6.32s\tremaining: 1m 59s\n600:\tlearn: 0.2059634\ttest: 0.2477302\tbest: 0.2477302 (600)\ttotal: 7.66s\tremaining: 1m 59s\n700:\tlearn: 0.1938203\ttest: 0.2417486\tbest: 0.2417486 (700)\ttotal: 8.96s\tremaining: 1m 58s\n800:\tlearn: 0.1832069\ttest: 0.2366192\tbest: 0.2366192 (800)\ttotal: 10.6s\tremaining: 2m 1s\n900:\tlearn: 0.1733144\ttest: 0.2317527\tbest: 0.2317527 (900)\ttotal: 11.9s\tremaining: 2m\n1000:\tlearn: 0.1650288\ttest: 0.2271176\tbest: 0.2270948 (999)\ttotal: 13.2s\tremaining: 1m 58s\n1100:\tlearn: 0.1570852\ttest: 0.2227911\tbest: 0.2227911 (1100)\ttotal: 14.5s\tremaining: 1m 57s\n1200:\tlearn: 0.1501229\ttest: 0.2194010\tbest: 0.2194010 (1200)\ttotal: 15.8s\tremaining: 1m 55s\n1300:\tlearn: 0.1433117\ttest: 0.2160190\tbest: 0.2160190 (1300)\ttotal: 17.1s\tremaining: 1m 54s\n1400:\tlearn: 0.1368307\ttest: 0.2124326\tbest: 0.2124326 (1400)\ttotal: 18.4s\tremaining: 1m 52s\n1500:\tlearn: 0.1311692\ttest: 0.2102048\tbest: 0.2101955 (1499)\ttotal: 19.7s\tremaining: 1m 51s\n1600:\tlearn: 0.1258556\ttest: 0.2081986\tbest: 0.2081986 (1600)\ttotal: 22s\tremaining: 1m 55s\n1700:\tlearn: 0.1210468\ttest: 0.2051627\tbest: 0.2051442 (1696)\ttotal: 23.4s\tremaining: 1m 53s\n1800:\tlearn: 0.1165616\ttest: 0.2031822\tbest: 0.2031822 (1800)\ttotal: 24.7s\tremaining: 1m 52s\n1900:\tlearn: 0.1120498\ttest: 0.2014773\tbest: 0.2014773 (1900)\ttotal: 26s\tremaining: 1m 50s\n2000:\tlearn: 0.1082792\ttest: 0.2001508\tbest: 0.2001415 (1997)\ttotal: 27.2s\tremaining: 1m 48s\n2100:\tlearn: 0.1043356\ttest: 0.1980494\tbest: 0.1980494 (2100)\ttotal: 28.5s\tremaining: 1m 47s\n2200:\tlearn: 0.1005875\ttest: 0.1963113\tbest: 0.1963113 (2200)\ttotal: 29.8s\tremaining: 1m 45s\n2300:\tlearn: 0.0973885\ttest: 0.1950364\tbest: 0.1949714 (2297)\ttotal: 31.2s\tremaining: 1m 44s\n2400:\tlearn: 0.0941804\ttest: 0.1943154\tbest: 0.1942956 (2394)\ttotal: 32.7s\tremaining: 1m 43s\n2500:\tlearn: 0.0910950\ttest: 0.1934267\tbest: 0.1934267 (2500)\ttotal: 33.9s\tremaining: 1m 41s\n2600:\tlearn: 0.0880828\ttest: 0.1922859\tbest: 0.1922859 (2600)\ttotal: 35.2s\tremaining: 1m 40s\n2700:\tlearn: 0.0853027\ttest: 0.1923069\tbest: 0.1920838 (2611)\ttotal: 36.5s\tremaining: 1m 38s\n2800:\tlearn: 0.0827277\ttest: 0.1916339\tbest: 0.1916000 (2799)\ttotal: 37.8s\tremaining: 1m 37s\n2900:\tlearn: 0.0800963\ttest: 0.1906552\tbest: 0.1905848 (2890)\ttotal: 39.1s\tremaining: 1m 35s\n3000:\tlearn: 0.0779003\ttest: 0.1902042\tbest: 0.1901940 (2998)\ttotal: 40.4s\tremaining: 1m 34s\n3100:\tlearn: 0.0756816\ttest: 0.1899266\tbest: 0.1898687 (3080)\ttotal: 41.7s\tremaining: 1m 32s\n3200:\tlearn: 0.0735576\ttest: 0.1890464\tbest: 0.1890464 (3200)\ttotal: 43.3s\tremaining: 1m 31s\n3300:\tlearn: 0.0714839\ttest: 0.1882430\tbest: 0.1882326 (3298)\ttotal: 44.6s\tremaining: 1m 30s\n3400:\tlearn: 0.0694978\ttest: 0.1879066\tbest: 0.1878369 (3393)\ttotal: 45.9s\tremaining: 1m 28s\n3500:\tlearn: 0.0677896\ttest: 0.1878082\tbest: 0.1878082 (3500)\ttotal: 47.1s\tremaining: 1m 27s\n3600:\tlearn: 0.0659911\ttest: 0.1874432\tbest: 0.1874077 (3572)\ttotal: 48.4s\tremaining: 1m 26s\n3700:\tlearn: 0.0643033\ttest: 0.1875928\tbest: 0.1873494 (3637)\ttotal: 49.7s\tremaining: 1m 24s\n3800:\tlearn: 0.0626178\ttest: 0.1873503\tbest: 0.1873243 (3798)\ttotal: 51s\tremaining: 1m 23s\n3900:\tlearn: 0.0610210\ttest: 0.1870203\tbest: 0.1870041 (3899)\ttotal: 53s\tremaining: 1m 22s\n4000:\tlearn: 0.0594317\ttest: 0.1866096\tbest: 0.1865229 (3989)\ttotal: 54.7s\tremaining: 1m 22s\n4100:\tlearn: 0.0579037\ttest: 0.1864170\tbest: 0.1864074 (4099)\ttotal: 56s\tremaining: 1m 20s\n4200:\tlearn: 0.0565430\ttest: 0.1866655\tbest: 0.1864041 (4103)\ttotal: 57.3s\tremaining: 1m 19s\n4300:\tlearn: 0.0551729\ttest: 0.1870313\tbest: 0.1864041 (4103)\ttotal: 58.6s\tremaining: 1m 17s\nStopped by overfitting detector  (200 iterations wait)\n\nbestTest = 0.186404129\nbestIteration = 4103\n\nShrink model to first 4104 iterations.\n0:\tlearn: 0.6529329\ttest: 0.6530308\tbest: 0.6530308 (0)\ttotal: 11.1ms\tremaining: 1m 50s\n100:\tlearn: 0.3051772\ttest: 0.3162391\tbest: 0.3162391 (100)\ttotal: 1.23s\tremaining: 2m\n200:\tlearn: 0.2803046\ttest: 0.3007795\tbest: 0.3007795 (200)\ttotal: 2.52s\tremaining: 2m 2s\n300:\tlearn: 0.2559776\ttest: 0.2861513\tbest: 0.2861513 (300)\ttotal: 3.78s\tremaining: 2m 1s\n400:\tlearn: 0.2361396\ttest: 0.2747743\tbest: 0.2747743 (400)\ttotal: 5.21s\tremaining: 2m 4s\n500:\tlearn: 0.2203532\ttest: 0.2653248\tbest: 0.2652996 (499)\ttotal: 6.67s\tremaining: 2m 6s\n600:\tlearn: 0.2057413\ttest: 0.2561849\tbest: 0.2561849 (600)\ttotal: 7.96s\tremaining: 2m 4s\n700:\tlearn: 0.1929856\ttest: 0.2485906\tbest: 0.2485225 (698)\ttotal: 9.23s\tremaining: 2m 2s\n800:\tlearn: 0.1816375\ttest: 0.2404989\tbest: 0.2404989 (800)\ttotal: 10.5s\tremaining: 2m 1s\n900:\tlearn: 0.1710901\ttest: 0.2343861\tbest: 0.2343861 (900)\ttotal: 11.9s\tremaining: 1m 59s\n1000:\tlearn: 0.1617994\ttest: 0.2294002\tbest: 0.2293829 (997)\ttotal: 13.2s\tremaining: 1m 58s\n1100:\tlearn: 0.1537315\ttest: 0.2253383\tbest: 0.2253383 (1100)\ttotal: 14.4s\tremaining: 1m 56s\n1200:\tlearn: 0.1467376\ttest: 0.2219309\tbest: 0.2219309 (1200)\ttotal: 15.9s\tremaining: 1m 56s\n1300:\tlearn: 0.1400529\ttest: 0.2193670\tbest: 0.2193233 (1298)\ttotal: 17.5s\tremaining: 1m 56s\n1400:\tlearn: 0.1332937\ttest: 0.2159505\tbest: 0.2159505 (1400)\ttotal: 18.8s\tremaining: 1m 55s\n1500:\tlearn: 0.1274215\ttest: 0.2139860\tbest: 0.2139157 (1491)\ttotal: 20.1s\tremaining: 1m 53s\n1600:\tlearn: 0.1223919\ttest: 0.2124116\tbest: 0.2123876 (1587)\ttotal: 21.4s\tremaining: 1m 52s\n1700:\tlearn: 0.1175711\ttest: 0.2109204\tbest: 0.2109204 (1700)\ttotal: 22.7s\tremaining: 1m 50s\n1800:\tlearn: 0.1125809\ttest: 0.2086680\tbest: 0.2086680 (1800)\ttotal: 24s\tremaining: 1m 49s\n1900:\tlearn: 0.1083114\ttest: 0.2071174\tbest: 0.2071079 (1898)\ttotal: 26.2s\tremaining: 1m 51s\n2000:\tlearn: 0.1043304\ttest: 0.2057101\tbest: 0.2056806 (1999)\ttotal: 27.8s\tremaining: 1m 51s\n2100:\tlearn: 0.1004598\ttest: 0.2048120\tbest: 0.2048001 (2099)\ttotal: 29.1s\tremaining: 1m 49s\n2200:\tlearn: 0.0970488\ttest: 0.2037751\tbest: 0.2036437 (2194)\ttotal: 30.4s\tremaining: 1m 47s\n2300:\tlearn: 0.0933872\ttest: 0.2023335\tbest: 0.2023334 (2298)\ttotal: 31.8s\tremaining: 1m 46s\n2400:\tlearn: 0.0902924\ttest: 0.2015270\tbest: 0.2015270 (2400)\ttotal: 33.1s\tremaining: 1m 44s\n2500:\tlearn: 0.0872643\ttest: 0.2010552\tbest: 0.2010552 (2500)\ttotal: 34.4s\tremaining: 1m 43s\n2600:\tlearn: 0.0845200\ttest: 0.2004855\tbest: 0.2004360 (2595)\ttotal: 35.7s\tremaining: 1m 41s\n2700:\tlearn: 0.0818423\ttest: 0.1999871\tbest: 0.1999674 (2695)\ttotal: 37s\tremaining: 1m 40s\n2800:\tlearn: 0.0794032\ttest: 0.1992736\tbest: 0.1992439 (2790)\ttotal: 38.7s\tremaining: 1m 39s\n2900:\tlearn: 0.0768162\ttest: 0.1986019\tbest: 0.1986019 (2900)\ttotal: 40s\tremaining: 1m 37s\n3000:\tlearn: 0.0744921\ttest: 0.1983161\tbest: 0.1981949 (2993)\ttotal: 41.3s\tremaining: 1m 36s\n3100:\tlearn: 0.0723220\ttest: 0.1977991\tbest: 0.1977750 (3098)\ttotal: 42.6s\tremaining: 1m 34s\n3200:\tlearn: 0.0702185\ttest: 0.1976728\tbest: 0.1975532 (3119)\ttotal: 43.8s\tremaining: 1m 33s\n3300:\tlearn: 0.0680119\ttest: 0.1973829\tbest: 0.1973283 (3296)\ttotal: 45.1s\tremaining: 1m 31s\n3400:\tlearn: 0.0661254\ttest: 0.1973859\tbest: 0.1973152 (3395)\ttotal: 46.4s\tremaining: 1m 29s\n3500:\tlearn: 0.0643151\ttest: 0.1975223\tbest: 0.1973152 (3395)\ttotal: 47.7s\tremaining: 1m 28s\n3600:\tlearn: 0.0623980\ttest: 0.1972720\tbest: 0.1972694 (3595)\ttotal: 49.3s\tremaining: 1m 27s\n3700:\tlearn: 0.0603901\ttest: 0.1973762\tbest: 0.1972345 (3681)\ttotal: 50.6s\tremaining: 1m 26s\n3800:\tlearn: 0.0588409\ttest: 0.1973155\tbest: 0.1970634 (3758)\ttotal: 51.9s\tremaining: 1m 24s\n3900:\tlearn: 0.0573510\ttest: 0.1976396\tbest: 0.1970432 (3824)\ttotal: 53.1s\tremaining: 1m 23s\n4000:\tlearn: 0.0559521\ttest: 0.1975160\tbest: 0.1970432 (3824)\ttotal: 54.4s\tremaining: 1m 21s\nStopped by overfitting detector  (200 iterations wait)\n\nbestTest = 0.1970431554\nbestIteration = 3824\n\nShrink model to first 3825 iterations.\n","output_type":"stream"}]},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nlgb = LGBMClassifier(learning_rate=0.05, n_estimators=10000)\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=32)\nresult_lgb = 0 \nbest_score_lgb=0 \n\nfor train_index,valid_index in skf.split(train2,train[\"전화해지여부\"]):\n    x_train= train2.iloc[train_index]\n    x_valid= train2.iloc[valid_index]\n    y_train= train[\"전화해지여부\"].iloc[train_index]\n    y_valid= train[\"전화해지여부\"].iloc[valid_index]\n    lgb.fit(np.array(x_train),y_train,eval_set=(np.array(x_valid),y_valid),early_stopping_rounds=200,verbose=200)\n    result_lgb+= lgb.predict_proba(test2) / 10\n    best_score_lgb+= lgb.best_score_[\"valid_0\"][\"binary_logloss\"] / 10 #이중딕셔너리로 되어있어서 껍데이 뜯어야함, #회기면 rmse,","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:44:01.598868Z","iopub.execute_input":"2023-03-26T20:44:01.599257Z","iopub.status.idle":"2023-03-26T20:45:15.693022Z","shell.execute_reply.started":"2023-03-26T20:44:01.599218Z","shell.execute_reply":"2023-03-26T20:45:15.692140Z"},"trusted":true},"execution_count":151,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[200]\tvalid_0's binary_logloss: 0.249054\n[400]\tvalid_0's binary_logloss: 0.220878\n[600]\tvalid_0's binary_logloss: 0.20686\n[800]\tvalid_0's binary_logloss: 0.198353\n[1000]\tvalid_0's binary_logloss: 0.194438\n[1200]\tvalid_0's binary_logloss: 0.191981\n[1400]\tvalid_0's binary_logloss: 0.192622\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[200]\tvalid_0's binary_logloss: 0.256684\n[400]\tvalid_0's binary_logloss: 0.228409\n[600]\tvalid_0's binary_logloss: 0.215257\n[800]\tvalid_0's binary_logloss: 0.207817\n[1000]\tvalid_0's binary_logloss: 0.204414\n[1200]\tvalid_0's binary_logloss: 0.204372\n[1400]\tvalid_0's binary_logloss: 0.204921\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[200]\tvalid_0's binary_logloss: 0.253501\n[400]\tvalid_0's binary_logloss: 0.226779\n[600]\tvalid_0's binary_logloss: 0.211193\n[800]\tvalid_0's binary_logloss: 0.204859\n[1000]\tvalid_0's binary_logloss: 0.201651\n[1200]\tvalid_0's binary_logloss: 0.199476\n[1400]\tvalid_0's binary_logloss: 0.201468\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[200]\tvalid_0's binary_logloss: 0.248118\n[400]\tvalid_0's binary_logloss: 0.218825\n[600]\tvalid_0's binary_logloss: 0.201056\n[800]\tvalid_0's binary_logloss: 0.190583\n[1000]\tvalid_0's binary_logloss: 0.186366\n[1200]\tvalid_0's binary_logloss: 0.184199\n[1400]\tvalid_0's binary_logloss: 0.183968\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[200]\tvalid_0's binary_logloss: 0.255096\n[400]\tvalid_0's binary_logloss: 0.228813\n[600]\tvalid_0's binary_logloss: 0.216152\n[800]\tvalid_0's binary_logloss: 0.207675\n[1000]\tvalid_0's binary_logloss: 0.204165\n[1200]\tvalid_0's binary_logloss: 0.204786\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[200]\tvalid_0's binary_logloss: 0.247975\n[400]\tvalid_0's binary_logloss: 0.220825\n[600]\tvalid_0's binary_logloss: 0.209802\n[800]\tvalid_0's binary_logloss: 0.203596\n[1000]\tvalid_0's binary_logloss: 0.201429\n[1200]\tvalid_0's binary_logloss: 0.200364\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[200]\tvalid_0's binary_logloss: 0.24896\n[400]\tvalid_0's binary_logloss: 0.221486\n[600]\tvalid_0's binary_logloss: 0.206833\n[800]\tvalid_0's binary_logloss: 0.199179\n[1000]\tvalid_0's binary_logloss: 0.19406\n[1200]\tvalid_0's binary_logloss: 0.193805\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[200]\tvalid_0's binary_logloss: 0.256813\n[400]\tvalid_0's binary_logloss: 0.231148\n[600]\tvalid_0's binary_logloss: 0.21692\n[800]\tvalid_0's binary_logloss: 0.211313\n[1000]\tvalid_0's binary_logloss: 0.20801\n[1200]\tvalid_0's binary_logloss: 0.206502\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[200]\tvalid_0's binary_logloss: 0.250454\n[400]\tvalid_0's binary_logloss: 0.225291\n[600]\tvalid_0's binary_logloss: 0.211461\n[800]\tvalid_0's binary_logloss: 0.20434\n[1000]\tvalid_0's binary_logloss: 0.200125\n[1200]\tvalid_0's binary_logloss: 0.198846\n[1400]\tvalid_0's binary_logloss: 0.199894\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"},{"name":"stdout","text":"[200]\tvalid_0's binary_logloss: 0.250098\n[400]\tvalid_0's binary_logloss: 0.221899\n[600]\tvalid_0's binary_logloss: 0.208081\n[800]\tvalid_0's binary_logloss: 0.198735\n[1000]\tvalid_0's binary_logloss: 0.194943\n[1200]\tvalid_0's binary_logloss: 0.193563\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\netc = ExtraTreesClassifier(n_estimators=900, n_jobs = -1)\netc.fit(np.array(train2), train[\"전화해지여부\"])\nresult_etc = etc.predict_proba(np.array(test2))\nresult_etc\n\n#배깅 평가셋 실시간 확인못함","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:45:15.697128Z","iopub.execute_input":"2023-03-26T20:45:15.699267Z","iopub.status.idle":"2023-03-26T20:45:40.473163Z","shell.execute_reply.started":"2023-03-26T20:45:15.699230Z","shell.execute_reply":"2023-03-26T20:45:40.471988Z"},"trusted":true},"execution_count":152,"outputs":[{"execution_count":152,"output_type":"execute_result","data":{"text/plain":"array([[0.90777778, 0.09222222],\n       [0.98888889, 0.01111111],\n       [0.93666667, 0.06333333],\n       ...,\n       [0.98333333, 0.01666667],\n       [0.99888889, 0.00111111],\n       [0.98444444, 0.01555556]])"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.layers import *\nfrom tensorflow.keras import *\nfrom tensorflow.keras.callbacks import *\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler #숫자적인 느낌으로 학습하니깐 범위가 어느정도 비슷해야함.\n\nss = StandardScaler()\n\nalldata_dl = ss.fit_transform(alldata2) \n\ntrain_dl = alldata_dl[:len(train)]\ntest_dl = alldata_dl[len(train):]\n\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=28)\nresult_dl = 0\nbest_score_dl = 0\n\nfor train_index, valid_index in skf.split(train_dl, train['전화해지여부']):\n    \n    x_train = train_dl[train_index]\n    x_valid = train_dl[valid_index]\n    \n    y_train = train['전화해지여부'].iloc[train_index]\n    y_valid = train['전화해지여부'].iloc[valid_index]\n    \n    model = Sequential()\n    model.add(Dense(1024, activation='relu', input_dim=train_dl.shape[1]))\n    model.add(BatchNormalization())\n    model.add(Dense(512,activation='elu'))\n    model.add(BatchNormalization())\n    model.add(Dense(128,activation='elu'))\n    model.add(BatchNormalization())\n    model.add(Dense(64,activation='elu'))\n    model.add(BatchNormalization())\n    model.add(Dense(32,activation='elu'))\n    model.add(BatchNormalization())\n    model.add(Dense(2,activation='softmax'))\n   \n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics='acc')\n    \n    es = EarlyStopping(patience=10, restore_best_weights=True)\n    model.fit(x_train,y_train, validation_data=(x_valid,y_valid), callbacks=[es], epochs=1000, batch_size=256)\n    \n    result_dl += model.predict(test_dl) / 10","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:45:40.474669Z","iopub.execute_input":"2023-03-26T20:45:40.476201Z","iopub.status.idle":"2023-03-26T20:52:24.308569Z","shell.execute_reply.started":"2023-03-26T20:45:40.476154Z","shell.execute_reply":"2023-03-26T20:52:24.307543Z"},"trusted":true},"execution_count":153,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str', 'tuple']. An error will be raised in 1.2.\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1000\n107/107 [==============================] - 6s 11ms/step - loss: 0.5223 - acc: 0.8071 - val_loss: 0.4010 - val_acc: 0.8904\nEpoch 2/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3442 - acc: 0.8903 - val_loss: 0.3382 - val_acc: 0.8904\nEpoch 3/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3221 - acc: 0.8903 - val_loss: 0.3298 - val_acc: 0.8904\nEpoch 4/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3162 - acc: 0.8912 - val_loss: 0.3193 - val_acc: 0.8901\nEpoch 5/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3078 - acc: 0.8922 - val_loss: 0.3295 - val_acc: 0.8930\nEpoch 6/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3040 - acc: 0.8925 - val_loss: 0.3134 - val_acc: 0.8927\nEpoch 7/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2952 - acc: 0.8932 - val_loss: 0.3053 - val_acc: 0.8967\nEpoch 8/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2916 - acc: 0.8925 - val_loss: 0.3135 - val_acc: 0.8921\nEpoch 9/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2844 - acc: 0.8940 - val_loss: 0.3057 - val_acc: 0.8937\nEpoch 10/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2768 - acc: 0.8957 - val_loss: 0.3024 - val_acc: 0.8950\nEpoch 11/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2717 - acc: 0.8975 - val_loss: 0.2921 - val_acc: 0.8960\nEpoch 12/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2669 - acc: 0.8983 - val_loss: 0.2907 - val_acc: 0.8980\nEpoch 13/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2585 - acc: 0.8994 - val_loss: 0.3021 - val_acc: 0.8877\nEpoch 14/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2570 - acc: 0.8994 - val_loss: 0.2926 - val_acc: 0.8947\nEpoch 15/1000\n107/107 [==============================] - 1s 8ms/step - loss: 0.2474 - acc: 0.9018 - val_loss: 0.2842 - val_acc: 0.8987\nEpoch 16/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2370 - acc: 0.9046 - val_loss: 0.2712 - val_acc: 0.9050\nEpoch 17/1000\n107/107 [==============================] - 1s 6ms/step - loss: 0.2341 - acc: 0.9057 - val_loss: 0.2761 - val_acc: 0.8967\nEpoch 18/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2251 - acc: 0.9092 - val_loss: 0.2779 - val_acc: 0.8957\nEpoch 19/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2241 - acc: 0.9103 - val_loss: 0.2744 - val_acc: 0.8993\nEpoch 20/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2118 - acc: 0.9139 - val_loss: 0.2876 - val_acc: 0.8848\nEpoch 21/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2075 - acc: 0.9156 - val_loss: 0.2805 - val_acc: 0.8974\nEpoch 22/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2032 - acc: 0.9174 - val_loss: 0.2615 - val_acc: 0.9063\nEpoch 23/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1973 - acc: 0.9195 - val_loss: 0.2697 - val_acc: 0.9033\nEpoch 24/1000\n107/107 [==============================] - 1s 6ms/step - loss: 0.1951 - acc: 0.9212 - val_loss: 0.2805 - val_acc: 0.8987\nEpoch 25/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1852 - acc: 0.9228 - val_loss: 0.2826 - val_acc: 0.8974\nEpoch 26/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1814 - acc: 0.9247 - val_loss: 0.2822 - val_acc: 0.9000\nEpoch 27/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1746 - acc: 0.9275 - val_loss: 0.2651 - val_acc: 0.9076\nEpoch 28/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1690 - acc: 0.9307 - val_loss: 0.2864 - val_acc: 0.9013\nEpoch 29/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1656 - acc: 0.9312 - val_loss: 0.2730 - val_acc: 0.8997\nEpoch 30/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.1593 - acc: 0.9329 - val_loss: 0.2793 - val_acc: 0.8993\nEpoch 31/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1568 - acc: 0.9352 - val_loss: 0.2634 - val_acc: 0.9030\nEpoch 32/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1559 - acc: 0.9338 - val_loss: 0.2637 - val_acc: 0.9073\n405/405 [==============================] - 1s 2ms/step\nEpoch 1/1000\n107/107 [==============================] - 5s 10ms/step - loss: 0.5343 - acc: 0.8104 - val_loss: 0.3991 - val_acc: 0.8904\nEpoch 2/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3485 - acc: 0.8905 - val_loss: 0.3411 - val_acc: 0.8904\nEpoch 3/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3223 - acc: 0.8914 - val_loss: 0.3247 - val_acc: 0.8904\nEpoch 4/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3159 - acc: 0.8917 - val_loss: 0.3185 - val_acc: 0.8917\nEpoch 5/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3100 - acc: 0.8929 - val_loss: 0.3141 - val_acc: 0.8930\nEpoch 6/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3050 - acc: 0.8931 - val_loss: 0.3066 - val_acc: 0.8937\nEpoch 7/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2998 - acc: 0.8932 - val_loss: 0.3014 - val_acc: 0.8940\nEpoch 8/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2945 - acc: 0.8932 - val_loss: 0.3072 - val_acc: 0.8934\nEpoch 9/1000\n107/107 [==============================] - 1s 6ms/step - loss: 0.2882 - acc: 0.8949 - val_loss: 0.3020 - val_acc: 0.8927\nEpoch 10/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2803 - acc: 0.8953 - val_loss: 0.3010 - val_acc: 0.8914\nEpoch 11/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2741 - acc: 0.8960 - val_loss: 0.2975 - val_acc: 0.8924\nEpoch 12/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2661 - acc: 0.8982 - val_loss: 0.2884 - val_acc: 0.8940\nEpoch 13/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2598 - acc: 0.8987 - val_loss: 0.3029 - val_acc: 0.8897\nEpoch 14/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2533 - acc: 0.9022 - val_loss: 0.2906 - val_acc: 0.8944\nEpoch 15/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.2463 - acc: 0.9038 - val_loss: 0.2831 - val_acc: 0.8901\nEpoch 16/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.2398 - acc: 0.9059 - val_loss: 0.2800 - val_acc: 0.8901\nEpoch 17/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.2305 - acc: 0.9089 - val_loss: 0.2768 - val_acc: 0.8964\nEpoch 18/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2242 - acc: 0.9102 - val_loss: 0.2937 - val_acc: 0.8947\nEpoch 19/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2204 - acc: 0.9115 - val_loss: 0.2653 - val_acc: 0.9007\nEpoch 20/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2126 - acc: 0.9136 - val_loss: 0.2702 - val_acc: 0.8967\nEpoch 21/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2045 - acc: 0.9170 - val_loss: 0.2730 - val_acc: 0.9050\nEpoch 22/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1997 - acc: 0.9199 - val_loss: 0.2559 - val_acc: 0.9083\nEpoch 23/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1907 - acc: 0.9215 - val_loss: 0.2805 - val_acc: 0.9013\nEpoch 24/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1851 - acc: 0.9242 - val_loss: 0.2570 - val_acc: 0.8983\nEpoch 25/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1787 - acc: 0.9273 - val_loss: 0.2688 - val_acc: 0.9026\nEpoch 26/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1755 - acc: 0.9278 - val_loss: 0.2623 - val_acc: 0.9000\nEpoch 27/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1720 - acc: 0.9277 - val_loss: 0.2641 - val_acc: 0.9010\nEpoch 28/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1624 - acc: 0.9318 - val_loss: 0.2563 - val_acc: 0.9053\nEpoch 29/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.1628 - acc: 0.9333 - val_loss: 0.2650 - val_acc: 0.9106\nEpoch 30/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1564 - acc: 0.9334 - val_loss: 0.2613 - val_acc: 0.9093\nEpoch 31/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1525 - acc: 0.9362 - val_loss: 0.2541 - val_acc: 0.9070\nEpoch 32/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1498 - acc: 0.9379 - val_loss: 0.2619 - val_acc: 0.9000\nEpoch 33/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1426 - acc: 0.9400 - val_loss: 0.2636 - val_acc: 0.9010\nEpoch 34/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1395 - acc: 0.9436 - val_loss: 0.2505 - val_acc: 0.9126\nEpoch 35/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1352 - acc: 0.9441 - val_loss: 0.2605 - val_acc: 0.9020\nEpoch 36/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1311 - acc: 0.9445 - val_loss: 0.2617 - val_acc: 0.9000\nEpoch 37/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1302 - acc: 0.9450 - val_loss: 0.2510 - val_acc: 0.9162\nEpoch 38/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1299 - acc: 0.9449 - val_loss: 0.2651 - val_acc: 0.9046\nEpoch 39/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1234 - acc: 0.9467 - val_loss: 0.2714 - val_acc: 0.9043\nEpoch 40/1000\n107/107 [==============================] - 1s 6ms/step - loss: 0.1206 - acc: 0.9485 - val_loss: 0.2682 - val_acc: 0.9030\nEpoch 41/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1184 - acc: 0.9501 - val_loss: 0.2776 - val_acc: 0.9030\nEpoch 42/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1210 - acc: 0.9489 - val_loss: 0.2719 - val_acc: 0.9060\nEpoch 43/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1169 - acc: 0.9493 - val_loss: 0.2608 - val_acc: 0.9089\nEpoch 44/1000\n107/107 [==============================] - 1s 8ms/step - loss: 0.1091 - acc: 0.9531 - val_loss: 0.2714 - val_acc: 0.9116\n405/405 [==============================] - 1s 2ms/step\nEpoch 1/1000\n107/107 [==============================] - 5s 9ms/step - loss: 0.5462 - acc: 0.8051 - val_loss: 0.3939 - val_acc: 0.8901\nEpoch 2/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3532 - acc: 0.8903 - val_loss: 0.3343 - val_acc: 0.8901\nEpoch 3/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3230 - acc: 0.8909 - val_loss: 0.3278 - val_acc: 0.8917\nEpoch 4/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3164 - acc: 0.8911 - val_loss: 0.3214 - val_acc: 0.8917\nEpoch 5/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3113 - acc: 0.8920 - val_loss: 0.3200 - val_acc: 0.8924\nEpoch 6/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3063 - acc: 0.8913 - val_loss: 0.3130 - val_acc: 0.8927\nEpoch 7/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.2989 - acc: 0.8933 - val_loss: 0.3185 - val_acc: 0.8901\nEpoch 8/1000\n107/107 [==============================] - 1s 11ms/step - loss: 0.2957 - acc: 0.8938 - val_loss: 0.3071 - val_acc: 0.8927\nEpoch 9/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2901 - acc: 0.8943 - val_loss: 0.3043 - val_acc: 0.8934\nEpoch 10/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2836 - acc: 0.8951 - val_loss: 0.3034 - val_acc: 0.8921\nEpoch 11/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2762 - acc: 0.8958 - val_loss: 0.3051 - val_acc: 0.8927\nEpoch 12/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2705 - acc: 0.8979 - val_loss: 0.2986 - val_acc: 0.8964\nEpoch 13/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2624 - acc: 0.8993 - val_loss: 0.2925 - val_acc: 0.8894\nEpoch 14/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2593 - acc: 0.9010 - val_loss: 0.2886 - val_acc: 0.8954\nEpoch 15/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2525 - acc: 0.9018 - val_loss: 0.2865 - val_acc: 0.8921\nEpoch 16/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2432 - acc: 0.9056 - val_loss: 0.2851 - val_acc: 0.8934\nEpoch 17/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2377 - acc: 0.9064 - val_loss: 0.2842 - val_acc: 0.8894\nEpoch 18/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2314 - acc: 0.9078 - val_loss: 0.2753 - val_acc: 0.8930\nEpoch 19/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2264 - acc: 0.9104 - val_loss: 0.2783 - val_acc: 0.8947\nEpoch 20/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2199 - acc: 0.9135 - val_loss: 0.2769 - val_acc: 0.8927\nEpoch 21/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2113 - acc: 0.9167 - val_loss: 0.2769 - val_acc: 0.8907\nEpoch 22/1000\n107/107 [==============================] - 1s 8ms/step - loss: 0.2059 - acc: 0.9185 - val_loss: 0.2686 - val_acc: 0.8960\nEpoch 23/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2024 - acc: 0.9185 - val_loss: 0.2746 - val_acc: 0.8921\nEpoch 24/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1905 - acc: 0.9237 - val_loss: 0.2797 - val_acc: 0.8874\nEpoch 25/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1904 - acc: 0.9221 - val_loss: 0.2729 - val_acc: 0.8967\nEpoch 26/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1814 - acc: 0.9264 - val_loss: 0.2866 - val_acc: 0.8950\nEpoch 27/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1786 - acc: 0.9278 - val_loss: 0.2712 - val_acc: 0.8950\nEpoch 28/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1745 - acc: 0.9298 - val_loss: 0.2831 - val_acc: 0.8964\nEpoch 29/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1684 - acc: 0.9294 - val_loss: 0.2778 - val_acc: 0.8950\nEpoch 30/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1642 - acc: 0.9336 - val_loss: 0.2786 - val_acc: 0.9010\nEpoch 31/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1569 - acc: 0.9361 - val_loss: 0.2723 - val_acc: 0.8987\nEpoch 32/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1546 - acc: 0.9355 - val_loss: 0.2813 - val_acc: 0.9026\n405/405 [==============================] - 1s 2ms/step\nEpoch 1/1000\n107/107 [==============================] - 4s 9ms/step - loss: 0.5386 - acc: 0.8049 - val_loss: 0.4040 - val_acc: 0.8901\nEpoch 2/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3534 - acc: 0.8896 - val_loss: 0.3312 - val_acc: 0.8901\nEpoch 3/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3223 - acc: 0.8912 - val_loss: 0.3241 - val_acc: 0.8904\nEpoch 4/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3160 - acc: 0.8911 - val_loss: 0.3165 - val_acc: 0.8904\nEpoch 5/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3088 - acc: 0.8925 - val_loss: 0.3118 - val_acc: 0.8904\nEpoch 6/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3024 - acc: 0.8923 - val_loss: 0.3139 - val_acc: 0.8924\nEpoch 7/1000\n107/107 [==============================] - 1s 8ms/step - loss: 0.2960 - acc: 0.8931 - val_loss: 0.3163 - val_acc: 0.8894\nEpoch 8/1000\n107/107 [==============================] - 1s 8ms/step - loss: 0.2884 - acc: 0.8941 - val_loss: 0.3099 - val_acc: 0.8930\nEpoch 9/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2849 - acc: 0.8944 - val_loss: 0.3034 - val_acc: 0.8887\nEpoch 10/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2771 - acc: 0.8962 - val_loss: 0.3005 - val_acc: 0.8934\nEpoch 11/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2711 - acc: 0.8958 - val_loss: 0.2954 - val_acc: 0.8954\nEpoch 12/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2639 - acc: 0.8986 - val_loss: 0.2938 - val_acc: 0.8921\nEpoch 13/1000\n107/107 [==============================] - 1s 6ms/step - loss: 0.2563 - acc: 0.9002 - val_loss: 0.3025 - val_acc: 0.8877\nEpoch 14/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2533 - acc: 0.9021 - val_loss: 0.2861 - val_acc: 0.8934\nEpoch 15/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2433 - acc: 0.9039 - val_loss: 0.2811 - val_acc: 0.8960\nEpoch 16/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2365 - acc: 0.9052 - val_loss: 0.2862 - val_acc: 0.8921\nEpoch 17/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2327 - acc: 0.9075 - val_loss: 0.2785 - val_acc: 0.8967\nEpoch 18/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2241 - acc: 0.9103 - val_loss: 0.2721 - val_acc: 0.8974\nEpoch 19/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2215 - acc: 0.9113 - val_loss: 0.2709 - val_acc: 0.8930\nEpoch 20/1000\n107/107 [==============================] - 1s 6ms/step - loss: 0.2130 - acc: 0.9139 - val_loss: 0.2740 - val_acc: 0.8980\nEpoch 21/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2029 - acc: 0.9170 - val_loss: 0.2812 - val_acc: 0.8940\nEpoch 22/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.1979 - acc: 0.9192 - val_loss: 0.2753 - val_acc: 0.9003\nEpoch 23/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1910 - acc: 0.9213 - val_loss: 0.2809 - val_acc: 0.8921\nEpoch 24/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1905 - acc: 0.9226 - val_loss: 0.2800 - val_acc: 0.8967\nEpoch 25/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1816 - acc: 0.9256 - val_loss: 0.2627 - val_acc: 0.9003\nEpoch 26/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1736 - acc: 0.9284 - val_loss: 0.2653 - val_acc: 0.8957\nEpoch 27/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1700 - acc: 0.9301 - val_loss: 0.2652 - val_acc: 0.9026\nEpoch 28/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1646 - acc: 0.9323 - val_loss: 0.2710 - val_acc: 0.8970\nEpoch 29/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1629 - acc: 0.9322 - val_loss: 0.2692 - val_acc: 0.9017\nEpoch 30/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1543 - acc: 0.9364 - val_loss: 0.2607 - val_acc: 0.9043\nEpoch 31/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1529 - acc: 0.9358 - val_loss: 0.2609 - val_acc: 0.9010\nEpoch 32/1000\n107/107 [==============================] - 1s 6ms/step - loss: 0.1429 - acc: 0.9398 - val_loss: 0.2617 - val_acc: 0.9079\nEpoch 33/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1430 - acc: 0.9409 - val_loss: 0.2595 - val_acc: 0.9076\nEpoch 34/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.1369 - acc: 0.9425 - val_loss: 0.2623 - val_acc: 0.9046\nEpoch 35/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1346 - acc: 0.9424 - val_loss: 0.2623 - val_acc: 0.8983\nEpoch 36/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1283 - acc: 0.9467 - val_loss: 0.2814 - val_acc: 0.9000\nEpoch 37/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.1283 - acc: 0.9457 - val_loss: 0.2832 - val_acc: 0.9007\nEpoch 38/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1277 - acc: 0.9477 - val_loss: 0.2730 - val_acc: 0.9036\nEpoch 39/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1227 - acc: 0.9477 - val_loss: 0.2754 - val_acc: 0.9099\nEpoch 40/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1181 - acc: 0.9496 - val_loss: 0.2734 - val_acc: 0.9066\nEpoch 41/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1142 - acc: 0.9503 - val_loss: 0.2836 - val_acc: 0.9066\nEpoch 42/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1212 - acc: 0.9486 - val_loss: 0.2751 - val_acc: 0.9060\nEpoch 43/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1112 - acc: 0.9522 - val_loss: 0.2776 - val_acc: 0.9093\n405/405 [==============================] - 1s 2ms/step\nEpoch 1/1000\n107/107 [==============================] - 4s 9ms/step - loss: 0.5369 - acc: 0.8045 - val_loss: 0.3927 - val_acc: 0.8901\nEpoch 2/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3480 - acc: 0.8907 - val_loss: 0.3371 - val_acc: 0.8901\nEpoch 3/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3195 - acc: 0.8919 - val_loss: 0.3283 - val_acc: 0.8904\nEpoch 4/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3141 - acc: 0.8920 - val_loss: 0.3285 - val_acc: 0.8907\nEpoch 5/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3054 - acc: 0.8919 - val_loss: 0.3295 - val_acc: 0.8907\nEpoch 6/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3007 - acc: 0.8930 - val_loss: 0.3284 - val_acc: 0.8881\nEpoch 7/1000\n107/107 [==============================] - 1s 8ms/step - loss: 0.2945 - acc: 0.8933 - val_loss: 0.3220 - val_acc: 0.8891\nEpoch 8/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2868 - acc: 0.8936 - val_loss: 0.3219 - val_acc: 0.8897\nEpoch 9/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2824 - acc: 0.8948 - val_loss: 0.3144 - val_acc: 0.8907\nEpoch 10/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2763 - acc: 0.8959 - val_loss: 0.3216 - val_acc: 0.8821\nEpoch 11/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2685 - acc: 0.8983 - val_loss: 0.3128 - val_acc: 0.8887\nEpoch 12/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2653 - acc: 0.8983 - val_loss: 0.3137 - val_acc: 0.8877\nEpoch 13/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2546 - acc: 0.9015 - val_loss: 0.3046 - val_acc: 0.8921\nEpoch 14/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2477 - acc: 0.9037 - val_loss: 0.3084 - val_acc: 0.8917\nEpoch 15/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2419 - acc: 0.9051 - val_loss: 0.3073 - val_acc: 0.8861\nEpoch 16/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.2369 - acc: 0.9056 - val_loss: 0.3001 - val_acc: 0.8887\nEpoch 17/1000\n107/107 [==============================] - 1s 10ms/step - loss: 0.2269 - acc: 0.9097 - val_loss: 0.2971 - val_acc: 0.8874\nEpoch 18/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2228 - acc: 0.9106 - val_loss: 0.2928 - val_acc: 0.8914\nEpoch 19/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2173 - acc: 0.9132 - val_loss: 0.2943 - val_acc: 0.8927\nEpoch 20/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2077 - acc: 0.9160 - val_loss: 0.2945 - val_acc: 0.8917\nEpoch 21/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.2046 - acc: 0.9179 - val_loss: 0.2790 - val_acc: 0.8964\nEpoch 22/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1933 - acc: 0.9213 - val_loss: 0.2919 - val_acc: 0.8937\nEpoch 23/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1928 - acc: 0.9206 - val_loss: 0.2884 - val_acc: 0.8944\nEpoch 24/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1878 - acc: 0.9234 - val_loss: 0.2952 - val_acc: 0.8887\nEpoch 25/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1768 - acc: 0.9290 - val_loss: 0.2921 - val_acc: 0.8997\nEpoch 26/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1719 - acc: 0.9297 - val_loss: 0.2839 - val_acc: 0.8997\nEpoch 27/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1679 - acc: 0.9299 - val_loss: 0.2910 - val_acc: 0.8993\nEpoch 28/1000\n107/107 [==============================] - 1s 6ms/step - loss: 0.1661 - acc: 0.9325 - val_loss: 0.2919 - val_acc: 0.9000\nEpoch 29/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1604 - acc: 0.9334 - val_loss: 0.2980 - val_acc: 0.8954\nEpoch 30/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1552 - acc: 0.9354 - val_loss: 0.2859 - val_acc: 0.9017\nEpoch 31/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1506 - acc: 0.9375 - val_loss: 0.2919 - val_acc: 0.9017\n405/405 [==============================] - 1s 2ms/step\nEpoch 1/1000\n107/107 [==============================] - 5s 12ms/step - loss: 0.5359 - acc: 0.8031 - val_loss: 0.4213 - val_acc: 0.8901\nEpoch 2/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3501 - acc: 0.8904 - val_loss: 0.3373 - val_acc: 0.8901\nEpoch 3/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3214 - acc: 0.8913 - val_loss: 0.3227 - val_acc: 0.8904\nEpoch 4/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3149 - acc: 0.8916 - val_loss: 0.3308 - val_acc: 0.8904\nEpoch 5/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3101 - acc: 0.8912 - val_loss: 0.3126 - val_acc: 0.8914\nEpoch 6/1000\n107/107 [==============================] - 1s 6ms/step - loss: 0.3053 - acc: 0.8921 - val_loss: 0.3142 - val_acc: 0.8937\nEpoch 7/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.2985 - acc: 0.8929 - val_loss: 0.3097 - val_acc: 0.8907\nEpoch 8/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2917 - acc: 0.8936 - val_loss: 0.3054 - val_acc: 0.8921\nEpoch 9/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2866 - acc: 0.8951 - val_loss: 0.3044 - val_acc: 0.8917\nEpoch 10/1000\n107/107 [==============================] - 1s 6ms/step - loss: 0.2816 - acc: 0.8945 - val_loss: 0.3054 - val_acc: 0.8927\nEpoch 11/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2736 - acc: 0.8971 - val_loss: 0.2943 - val_acc: 0.8940\nEpoch 12/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2659 - acc: 0.8996 - val_loss: 0.3023 - val_acc: 0.8868\nEpoch 13/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2620 - acc: 0.8992 - val_loss: 0.3017 - val_acc: 0.8907\nEpoch 14/1000\n107/107 [==============================] - 1s 6ms/step - loss: 0.2543 - acc: 0.9007 - val_loss: 0.2962 - val_acc: 0.8934\nEpoch 15/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2496 - acc: 0.9030 - val_loss: 0.2791 - val_acc: 0.8917\nEpoch 16/1000\n107/107 [==============================] - 1s 6ms/step - loss: 0.2427 - acc: 0.9057 - val_loss: 0.2883 - val_acc: 0.8924\nEpoch 17/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2346 - acc: 0.9070 - val_loss: 0.2866 - val_acc: 0.8811\nEpoch 18/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2269 - acc: 0.9096 - val_loss: 0.2770 - val_acc: 0.8911\nEpoch 19/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2182 - acc: 0.9146 - val_loss: 0.2780 - val_acc: 0.8954\nEpoch 20/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2145 - acc: 0.9147 - val_loss: 0.2839 - val_acc: 0.8924\nEpoch 21/1000\n107/107 [==============================] - 1s 6ms/step - loss: 0.2055 - acc: 0.9191 - val_loss: 0.2882 - val_acc: 0.8947\nEpoch 22/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.2007 - acc: 0.9174 - val_loss: 0.2880 - val_acc: 0.8894\nEpoch 23/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1951 - acc: 0.9217 - val_loss: 0.2778 - val_acc: 0.8957\nEpoch 24/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1917 - acc: 0.9229 - val_loss: 0.2785 - val_acc: 0.8957\nEpoch 25/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1828 - acc: 0.9251 - val_loss: 0.2738 - val_acc: 0.8957\nEpoch 26/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1801 - acc: 0.9265 - val_loss: 0.2720 - val_acc: 0.8950\nEpoch 27/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1730 - acc: 0.9290 - val_loss: 0.2767 - val_acc: 0.8954\nEpoch 28/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1706 - acc: 0.9306 - val_loss: 0.2839 - val_acc: 0.8947\nEpoch 29/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1645 - acc: 0.9330 - val_loss: 0.2890 - val_acc: 0.8947\nEpoch 30/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1603 - acc: 0.9343 - val_loss: 0.2751 - val_acc: 0.9013\nEpoch 31/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1524 - acc: 0.9383 - val_loss: 0.2767 - val_acc: 0.9017\nEpoch 32/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1481 - acc: 0.9378 - val_loss: 0.2804 - val_acc: 0.9079\nEpoch 33/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1512 - acc: 0.9375 - val_loss: 0.2797 - val_acc: 0.9000\nEpoch 34/1000\n107/107 [==============================] - 1s 6ms/step - loss: 0.1437 - acc: 0.9411 - val_loss: 0.2801 - val_acc: 0.9043\nEpoch 35/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1431 - acc: 0.9400 - val_loss: 0.2765 - val_acc: 0.9060\nEpoch 36/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1337 - acc: 0.9430 - val_loss: 0.2754 - val_acc: 0.9026\n405/405 [==============================] - 1s 2ms/step\nEpoch 1/1000\n107/107 [==============================] - 6s 10ms/step - loss: 0.5373 - acc: 0.8061 - val_loss: 0.4137 - val_acc: 0.8901\nEpoch 2/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3492 - acc: 0.8903 - val_loss: 0.3340 - val_acc: 0.8901\nEpoch 3/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3222 - acc: 0.8905 - val_loss: 0.3252 - val_acc: 0.8901\nEpoch 4/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3141 - acc: 0.8907 - val_loss: 0.3217 - val_acc: 0.8907\nEpoch 5/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3083 - acc: 0.8912 - val_loss: 0.3186 - val_acc: 0.8914\nEpoch 6/1000\n107/107 [==============================] - 1s 8ms/step - loss: 0.3004 - acc: 0.8927 - val_loss: 0.3152 - val_acc: 0.8924\nEpoch 7/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2956 - acc: 0.8934 - val_loss: 0.3115 - val_acc: 0.8897\nEpoch 8/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2903 - acc: 0.8935 - val_loss: 0.3048 - val_acc: 0.8904\nEpoch 9/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2836 - acc: 0.8937 - val_loss: 0.3139 - val_acc: 0.8887\nEpoch 10/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2760 - acc: 0.8955 - val_loss: 0.3011 - val_acc: 0.8874\nEpoch 11/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2692 - acc: 0.8961 - val_loss: 0.2974 - val_acc: 0.8884\nEpoch 12/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2656 - acc: 0.8977 - val_loss: 0.3028 - val_acc: 0.8887\nEpoch 13/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2554 - acc: 0.9003 - val_loss: 0.2924 - val_acc: 0.8874\nEpoch 14/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2490 - acc: 0.9014 - val_loss: 0.2846 - val_acc: 0.8960\nEpoch 15/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2424 - acc: 0.9053 - val_loss: 0.2883 - val_acc: 0.8921\nEpoch 16/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2371 - acc: 0.9053 - val_loss: 0.2844 - val_acc: 0.8904\nEpoch 17/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2248 - acc: 0.9107 - val_loss: 0.2756 - val_acc: 0.8937\nEpoch 18/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2249 - acc: 0.9103 - val_loss: 0.2773 - val_acc: 0.8877\nEpoch 19/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2120 - acc: 0.9146 - val_loss: 0.2728 - val_acc: 0.8997\nEpoch 20/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2118 - acc: 0.9142 - val_loss: 0.2711 - val_acc: 0.8977\nEpoch 21/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.2053 - acc: 0.9178 - val_loss: 0.2622 - val_acc: 0.8980\nEpoch 22/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2027 - acc: 0.9192 - val_loss: 0.2574 - val_acc: 0.9013\nEpoch 23/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1895 - acc: 0.9234 - val_loss: 0.2688 - val_acc: 0.9003\nEpoch 24/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1858 - acc: 0.9253 - val_loss: 0.2783 - val_acc: 0.8957\nEpoch 25/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1833 - acc: 0.9265 - val_loss: 0.2752 - val_acc: 0.9017\nEpoch 26/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1766 - acc: 0.9284 - val_loss: 0.2706 - val_acc: 0.8940\nEpoch 27/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1678 - acc: 0.9311 - val_loss: 0.2746 - val_acc: 0.8977\nEpoch 28/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1680 - acc: 0.9310 - val_loss: 0.2683 - val_acc: 0.9017\nEpoch 29/1000\n107/107 [==============================] - 1s 6ms/step - loss: 0.1641 - acc: 0.9303 - val_loss: 0.2702 - val_acc: 0.9026\nEpoch 30/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1554 - acc: 0.9347 - val_loss: 0.2640 - val_acc: 0.9013\nEpoch 31/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1521 - acc: 0.9378 - val_loss: 0.2702 - val_acc: 0.9036\nEpoch 32/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1441 - acc: 0.9406 - val_loss: 0.2546 - val_acc: 0.9089\nEpoch 33/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1446 - acc: 0.9385 - val_loss: 0.2595 - val_acc: 0.9063\nEpoch 34/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1375 - acc: 0.9409 - val_loss: 0.2613 - val_acc: 0.9089\nEpoch 35/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1368 - acc: 0.9433 - val_loss: 0.2652 - val_acc: 0.9106\nEpoch 36/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.1343 - acc: 0.9441 - val_loss: 0.2628 - val_acc: 0.9123\nEpoch 37/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1325 - acc: 0.9448 - val_loss: 0.2678 - val_acc: 0.9070\nEpoch 38/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1291 - acc: 0.9445 - val_loss: 0.2740 - val_acc: 0.9076\nEpoch 39/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1205 - acc: 0.9482 - val_loss: 0.2764 - val_acc: 0.9089\nEpoch 40/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1229 - acc: 0.9486 - val_loss: 0.2712 - val_acc: 0.9103\nEpoch 41/1000\n107/107 [==============================] - 1s 10ms/step - loss: 0.1206 - acc: 0.9483 - val_loss: 0.2823 - val_acc: 0.9003\nEpoch 42/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1184 - acc: 0.9485 - val_loss: 0.2773 - val_acc: 0.9073\n405/405 [==============================] - 1s 2ms/step\nEpoch 1/1000\n107/107 [==============================] - 5s 9ms/step - loss: 0.5400 - acc: 0.8058 - val_loss: 0.3962 - val_acc: 0.8901\nEpoch 2/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3500 - acc: 0.8899 - val_loss: 0.3304 - val_acc: 0.8901\nEpoch 3/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3214 - acc: 0.8915 - val_loss: 0.3279 - val_acc: 0.8901\nEpoch 4/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3137 - acc: 0.8917 - val_loss: 0.3202 - val_acc: 0.8901\nEpoch 5/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3065 - acc: 0.8926 - val_loss: 0.3139 - val_acc: 0.8894\nEpoch 6/1000\n107/107 [==============================] - 1s 8ms/step - loss: 0.2991 - acc: 0.8933 - val_loss: 0.3101 - val_acc: 0.8904\nEpoch 7/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2936 - acc: 0.8932 - val_loss: 0.3095 - val_acc: 0.8911\nEpoch 8/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2882 - acc: 0.8944 - val_loss: 0.3013 - val_acc: 0.8907\nEpoch 9/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2791 - acc: 0.8960 - val_loss: 0.3088 - val_acc: 0.8911\nEpoch 10/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2717 - acc: 0.8959 - val_loss: 0.3060 - val_acc: 0.8901\nEpoch 11/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2648 - acc: 0.8987 - val_loss: 0.2991 - val_acc: 0.8907\nEpoch 12/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2622 - acc: 0.8989 - val_loss: 0.3004 - val_acc: 0.8921\nEpoch 13/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2519 - acc: 0.9013 - val_loss: 0.2922 - val_acc: 0.8901\nEpoch 14/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2480 - acc: 0.9016 - val_loss: 0.2944 - val_acc: 0.8917\nEpoch 15/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2415 - acc: 0.9046 - val_loss: 0.2819 - val_acc: 0.8960\nEpoch 16/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2316 - acc: 0.9077 - val_loss: 0.2835 - val_acc: 0.8967\nEpoch 17/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2266 - acc: 0.9104 - val_loss: 0.2827 - val_acc: 0.8917\nEpoch 18/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2185 - acc: 0.9121 - val_loss: 0.2877 - val_acc: 0.8871\nEpoch 19/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2140 - acc: 0.9127 - val_loss: 0.2804 - val_acc: 0.8967\nEpoch 20/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2071 - acc: 0.9156 - val_loss: 0.2829 - val_acc: 0.8947\nEpoch 21/1000\n107/107 [==============================] - 1s 8ms/step - loss: 0.2025 - acc: 0.9166 - val_loss: 0.2899 - val_acc: 0.8934\nEpoch 22/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1970 - acc: 0.9183 - val_loss: 0.2766 - val_acc: 0.8954\nEpoch 23/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1859 - acc: 0.9244 - val_loss: 0.2591 - val_acc: 0.9020\nEpoch 24/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.1833 - acc: 0.9240 - val_loss: 0.2800 - val_acc: 0.8954\nEpoch 25/1000\n107/107 [==============================] - 1s 8ms/step - loss: 0.1789 - acc: 0.9245 - val_loss: 0.2761 - val_acc: 0.9020\nEpoch 26/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1742 - acc: 0.9283 - val_loss: 0.2756 - val_acc: 0.9007\nEpoch 27/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1674 - acc: 0.9305 - val_loss: 0.2763 - val_acc: 0.9007\nEpoch 28/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1623 - acc: 0.9318 - val_loss: 0.2645 - val_acc: 0.9096\nEpoch 29/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1572 - acc: 0.9344 - val_loss: 0.2765 - val_acc: 0.9007\nEpoch 30/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1495 - acc: 0.9370 - val_loss: 0.2643 - val_acc: 0.9063\nEpoch 31/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1510 - acc: 0.9371 - val_loss: 0.2747 - val_acc: 0.9046\nEpoch 32/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1451 - acc: 0.9394 - val_loss: 0.2811 - val_acc: 0.9046\nEpoch 33/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1387 - acc: 0.9402 - val_loss: 0.2651 - val_acc: 0.9026\n405/405 [==============================] - 1s 2ms/step\nEpoch 1/1000\n107/107 [==============================] - 5s 9ms/step - loss: 0.5420 - acc: 0.7972 - val_loss: 0.3999 - val_acc: 0.8901\nEpoch 2/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3515 - acc: 0.8906 - val_loss: 0.3305 - val_acc: 0.8901\nEpoch 3/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3213 - acc: 0.8906 - val_loss: 0.3219 - val_acc: 0.8901\nEpoch 4/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3144 - acc: 0.8916 - val_loss: 0.3180 - val_acc: 0.8911\nEpoch 5/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3081 - acc: 0.8922 - val_loss: 0.3096 - val_acc: 0.8930\nEpoch 6/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3031 - acc: 0.8929 - val_loss: 0.3087 - val_acc: 0.8897\nEpoch 7/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2955 - acc: 0.8924 - val_loss: 0.3069 - val_acc: 0.8940\nEpoch 8/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2898 - acc: 0.8931 - val_loss: 0.3002 - val_acc: 0.8950\nEpoch 9/1000\n107/107 [==============================] - 1s 8ms/step - loss: 0.2827 - acc: 0.8946 - val_loss: 0.3005 - val_acc: 0.8947\nEpoch 10/1000\n107/107 [==============================] - 1s 8ms/step - loss: 0.2759 - acc: 0.8969 - val_loss: 0.3038 - val_acc: 0.8927\nEpoch 11/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2693 - acc: 0.8976 - val_loss: 0.2953 - val_acc: 0.8921\nEpoch 12/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2605 - acc: 0.8994 - val_loss: 0.3016 - val_acc: 0.8887\nEpoch 13/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2565 - acc: 0.8999 - val_loss: 0.2931 - val_acc: 0.8911\nEpoch 14/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2488 - acc: 0.9011 - val_loss: 0.2875 - val_acc: 0.8937\nEpoch 15/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2404 - acc: 0.9057 - val_loss: 0.2876 - val_acc: 0.8874\nEpoch 16/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2349 - acc: 0.9058 - val_loss: 0.2914 - val_acc: 0.8937\nEpoch 17/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2308 - acc: 0.9086 - val_loss: 0.2794 - val_acc: 0.8937\nEpoch 18/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2203 - acc: 0.9118 - val_loss: 0.2796 - val_acc: 0.8977\nEpoch 19/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2160 - acc: 0.9151 - val_loss: 0.2815 - val_acc: 0.8983\nEpoch 20/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2048 - acc: 0.9173 - val_loss: 0.2820 - val_acc: 0.8974\nEpoch 21/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2015 - acc: 0.9187 - val_loss: 0.2723 - val_acc: 0.8974\nEpoch 22/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1924 - acc: 0.9217 - val_loss: 0.2728 - val_acc: 0.8983\nEpoch 23/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1877 - acc: 0.9243 - val_loss: 0.2713 - val_acc: 0.8967\nEpoch 24/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.1858 - acc: 0.9230 - val_loss: 0.2713 - val_acc: 0.8993\nEpoch 25/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1743 - acc: 0.9284 - val_loss: 0.2712 - val_acc: 0.8947\nEpoch 26/1000\n107/107 [==============================] - 1s 10ms/step - loss: 0.1739 - acc: 0.9283 - val_loss: 0.2839 - val_acc: 0.8970\nEpoch 27/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1653 - acc: 0.9322 - val_loss: 0.2749 - val_acc: 0.8993\nEpoch 28/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1569 - acc: 0.9347 - val_loss: 0.2811 - val_acc: 0.9000\nEpoch 29/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1581 - acc: 0.9346 - val_loss: 0.2842 - val_acc: 0.9046\nEpoch 30/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1516 - acc: 0.9357 - val_loss: 0.2711 - val_acc: 0.8964\nEpoch 31/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1461 - acc: 0.9381 - val_loss: 0.2779 - val_acc: 0.9040\nEpoch 32/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1395 - acc: 0.9427 - val_loss: 0.2855 - val_acc: 0.9023\nEpoch 33/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1444 - acc: 0.9392 - val_loss: 0.2828 - val_acc: 0.8993\nEpoch 34/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1375 - acc: 0.9435 - val_loss: 0.2906 - val_acc: 0.9000\nEpoch 35/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1345 - acc: 0.9442 - val_loss: 0.2901 - val_acc: 0.9026\nEpoch 36/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1311 - acc: 0.9444 - val_loss: 0.3034 - val_acc: 0.9003\nEpoch 37/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1293 - acc: 0.9464 - val_loss: 0.2977 - val_acc: 0.9070\nEpoch 38/1000\n107/107 [==============================] - 1s 8ms/step - loss: 0.1231 - acc: 0.9490 - val_loss: 0.2811 - val_acc: 0.9083\nEpoch 39/1000\n107/107 [==============================] - 1s 8ms/step - loss: 0.1161 - acc: 0.9514 - val_loss: 0.3093 - val_acc: 0.9007\nEpoch 40/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1213 - acc: 0.9492 - val_loss: 0.2902 - val_acc: 0.9036\n405/405 [==============================] - 1s 2ms/step\nEpoch 1/1000\n107/107 [==============================] - 5s 9ms/step - loss: 0.5342 - acc: 0.8121 - val_loss: 0.3912 - val_acc: 0.8901\nEpoch 2/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3494 - acc: 0.8905 - val_loss: 0.3434 - val_acc: 0.8911\nEpoch 3/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3218 - acc: 0.8915 - val_loss: 0.3279 - val_acc: 0.8907\nEpoch 4/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3150 - acc: 0.8920 - val_loss: 0.3195 - val_acc: 0.8927\nEpoch 5/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.3047 - acc: 0.8927 - val_loss: 0.3350 - val_acc: 0.8838\nEpoch 6/1000\n107/107 [==============================] - 1s 9ms/step - loss: 0.3032 - acc: 0.8923 - val_loss: 0.3239 - val_acc: 0.8901\nEpoch 7/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2948 - acc: 0.8930 - val_loss: 0.3257 - val_acc: 0.8877\nEpoch 8/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2896 - acc: 0.8947 - val_loss: 0.3180 - val_acc: 0.8881\nEpoch 9/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2832 - acc: 0.8953 - val_loss: 0.3158 - val_acc: 0.8891\nEpoch 10/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2800 - acc: 0.8963 - val_loss: 0.3132 - val_acc: 0.8894\nEpoch 11/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2715 - acc: 0.8971 - val_loss: 0.3162 - val_acc: 0.8924\nEpoch 12/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2653 - acc: 0.8987 - val_loss: 0.3125 - val_acc: 0.8897\nEpoch 13/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2609 - acc: 0.9003 - val_loss: 0.3034 - val_acc: 0.8884\nEpoch 14/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2561 - acc: 0.9016 - val_loss: 0.2987 - val_acc: 0.8854\nEpoch 15/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2451 - acc: 0.9060 - val_loss: 0.3018 - val_acc: 0.8864\nEpoch 16/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2382 - acc: 0.9065 - val_loss: 0.2983 - val_acc: 0.8927\nEpoch 17/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2347 - acc: 0.9068 - val_loss: 0.3060 - val_acc: 0.8854\nEpoch 18/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2292 - acc: 0.9106 - val_loss: 0.2898 - val_acc: 0.8877\nEpoch 19/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2204 - acc: 0.9136 - val_loss: 0.2892 - val_acc: 0.8884\nEpoch 20/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.2168 - acc: 0.9126 - val_loss: 0.2940 - val_acc: 0.8897\nEpoch 21/1000\n107/107 [==============================] - 1s 12ms/step - loss: 0.2055 - acc: 0.9177 - val_loss: 0.2997 - val_acc: 0.8930\nEpoch 22/1000\n107/107 [==============================] - 1s 8ms/step - loss: 0.2027 - acc: 0.9175 - val_loss: 0.2807 - val_acc: 0.8940\nEpoch 23/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1929 - acc: 0.9225 - val_loss: 0.2729 - val_acc: 0.8957\nEpoch 24/1000\n107/107 [==============================] - 1s 6ms/step - loss: 0.1919 - acc: 0.9238 - val_loss: 0.2841 - val_acc: 0.8993\nEpoch 25/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1840 - acc: 0.9260 - val_loss: 0.2743 - val_acc: 0.8947\nEpoch 26/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1752 - acc: 0.9281 - val_loss: 0.2648 - val_acc: 0.9020\nEpoch 27/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1705 - acc: 0.9305 - val_loss: 0.2894 - val_acc: 0.8934\nEpoch 28/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1664 - acc: 0.9313 - val_loss: 0.2788 - val_acc: 0.8967\nEpoch 29/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1609 - acc: 0.9324 - val_loss: 0.2709 - val_acc: 0.8990\nEpoch 30/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1579 - acc: 0.9334 - val_loss: 0.2730 - val_acc: 0.9017\nEpoch 31/1000\n107/107 [==============================] - 1s 6ms/step - loss: 0.1505 - acc: 0.9369 - val_loss: 0.2883 - val_acc: 0.8967\nEpoch 32/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1458 - acc: 0.9391 - val_loss: 0.2731 - val_acc: 0.8974\nEpoch 33/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1402 - acc: 0.9418 - val_loss: 0.2874 - val_acc: 0.9000\nEpoch 34/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1452 - acc: 0.9403 - val_loss: 0.2974 - val_acc: 0.8990\nEpoch 35/1000\n107/107 [==============================] - 1s 8ms/step - loss: 0.1380 - acc: 0.9422 - val_loss: 0.2837 - val_acc: 0.9033\nEpoch 36/1000\n107/107 [==============================] - 1s 7ms/step - loss: 0.1333 - acc: 0.9443 - val_loss: 0.2870 - val_acc: 0.8957\n405/405 [==============================] - 1s 2ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"final_result = result * 0.5 + result_lgb*0.4 + result_etc*0.07 + result_dl*0.03","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:58:16.046158Z","iopub.execute_input":"2023-03-26T20:58:16.047169Z","iopub.status.idle":"2023-03-26T20:58:16.054062Z","shell.execute_reply.started":"2023-03-26T20:58:16.047115Z","shell.execute_reply":"2023-03-26T20:58:16.052972Z"},"trusted":true},"execution_count":161,"outputs":[]},{"cell_type":"code","source":"sub=pd.read_csv(\"/kaggle/input/call-prediction/sample_submission.csv\")\nsub","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:58:16.057764Z","iopub.execute_input":"2023-03-26T20:58:16.058447Z","iopub.status.idle":"2023-03-26T20:58:16.083934Z","shell.execute_reply.started":"2023-03-26T20:58:16.058408Z","shell.execute_reply":"2023-03-26T20:58:16.082826Z"},"trusted":true},"execution_count":162,"outputs":[{"execution_count":162,"output_type":"execute_result","data":{"text/plain":"               ID  전화해지여부\n0      TEST_00000       0\n1      TEST_00001       0\n2      TEST_00002       0\n3      TEST_00003       0\n4      TEST_00004       0\n...           ...     ...\n12938  TEST_12938       0\n12939  TEST_12939       0\n12940  TEST_12940       0\n12941  TEST_12941       0\n12942  TEST_12942       0\n\n[12943 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>전화해지여부</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TEST_00000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TEST_00001</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TEST_00002</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TEST_00003</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TEST_00004</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12938</th>\n      <td>TEST_12938</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12939</th>\n      <td>TEST_12939</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12940</th>\n      <td>TEST_12940</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12941</th>\n      <td>TEST_12941</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12942</th>\n      <td>TEST_12942</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>12943 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sub[\"전화해지여부\"] = final_result[:,1] > 0.2\nsub","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:58:16.086029Z","iopub.execute_input":"2023-03-26T20:58:16.086436Z","iopub.status.idle":"2023-03-26T20:58:16.101059Z","shell.execute_reply.started":"2023-03-26T20:58:16.086400Z","shell.execute_reply":"2023-03-26T20:58:16.099976Z"},"trusted":true},"execution_count":163,"outputs":[{"execution_count":163,"output_type":"execute_result","data":{"text/plain":"               ID  전화해지여부\n0      TEST_00000   False\n1      TEST_00001   False\n2      TEST_00002   False\n3      TEST_00003    True\n4      TEST_00004    True\n...           ...     ...\n12938  TEST_12938   False\n12939  TEST_12939   False\n12940  TEST_12940   False\n12941  TEST_12941   False\n12942  TEST_12942   False\n\n[12943 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>전화해지여부</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TEST_00000</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TEST_00001</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TEST_00002</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TEST_00003</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TEST_00004</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12938</th>\n      <td>TEST_12938</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>12939</th>\n      <td>TEST_12939</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>12940</th>\n      <td>TEST_12940</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>12941</th>\n      <td>TEST_12941</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>12942</th>\n      <td>TEST_12942</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>12943 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sub[\"전화해지여부\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:58:16.102816Z","iopub.execute_input":"2023-03-26T20:58:16.103329Z","iopub.status.idle":"2023-03-26T20:58:16.112757Z","shell.execute_reply.started":"2023-03-26T20:58:16.103292Z","shell.execute_reply":"2023-03-26T20:58:16.111585Z"},"trusted":true},"execution_count":164,"outputs":[{"execution_count":164,"output_type":"execute_result","data":{"text/plain":"False    11356\nTrue      1587\nName: 전화해지여부, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"sub.to_csv(\"call_prediction1.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:58:16.115538Z","iopub.execute_input":"2023-03-26T20:58:16.116458Z","iopub.status.idle":"2023-03-26T20:58:16.135177Z","shell.execute_reply.started":"2023-03-26T20:58:16.116418Z","shell.execute_reply":"2023-03-26T20:58:16.134274Z"},"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import seaborn as sns\n# plt.figure(figsize=(16,8))\n# sns.boxplot(data=alldata,x=\"전화해지여부\",y=\"상담전화건수\",showfliers=False,)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:58:16.136431Z","iopub.execute_input":"2023-03-26T20:58:16.136980Z","iopub.status.idle":"2023-03-26T20:58:16.142448Z","shell.execute_reply.started":"2023-03-26T20:58:16.136940Z","shell.execute_reply":"2023-03-26T20:58:16.141145Z"},"trusted":true},"execution_count":166,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(16,8))\n# sns.boxplot(data=alldata,x=\"전화해지여부\",y=\"음성사서함이용\",hue=\"상담전화여부\",showfliers=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:58:16.144329Z","iopub.execute_input":"2023-03-26T20:58:16.145177Z","iopub.status.idle":"2023-03-26T20:58:16.151324Z","shell.execute_reply.started":"2023-03-26T20:58:16.145131Z","shell.execute_reply":"2023-03-26T20:58:16.150297Z"},"trusted":true},"execution_count":167,"outputs":[]}]}